{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced short question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced short question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you prove to a skeptical alien that you truly understand the concept of \"red\" without using human language or showing visual examples?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - update since the videos\n",
    "\n",
    "I've updated the model names to use the latest models below, like GPT 5 and Claude Sonnet 4.5. It's worth noting that these models can be quite slow - like 1-2 minutes - but they do a great job! Feel free to switch them for faster models if you'd prefer, like the ones I use in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - Skip these following steps if you want to execute your (Ehsan's concurrent execution of the models, which enables parallelization) [Jump to Exercise](#ehsan-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well\n",
    "# I've updated this with the latest model, but it can take some time because it likes to think!\n",
    "# Replace the model with gpt-4.1-mini if you'd prefer not to wait 1-2 mins\n",
    "\n",
    "model_name = \"gpt-5-nano\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-sonnet-4-5\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: How OpenAI SDK compatibility with Gemini works:\n",
    "# - SDK: You use the standard openai Python SDK (OpenAI library).\n",
    "# - Endpoint: Set base_url to https://generativelanguage.googleapis.com/v1beta/openai/ to redirect requests to Gemini's servers.\n",
    "# - API Key: Use your Gemini API Key from Google AI Studio (not an OpenAI key).\n",
    "# This compatibility is intentional, allowing developers to test or switch to Gemini models with minimal code changes, using the same OpenAI-based codebase.\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep-Seek\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://99.251.9.3:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "# print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "```\n",
    "{together}\n",
    "```\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "print(ranks)\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")\n",
    "    \n",
    "    \n",
    "# Ehsan added this to make it a oneliner.    \n",
    "# [ WHAT_I_WANT_TO_SAVE ] for [ TEMPORARY_VARIABLE_NAME ] in [ THE_LIST ]\n",
    "ranked = {f\"Rank {idx+1}\":competitors[int(rank)-1] for idx, rank in enumerate(ranks)}\n",
    "ranked = json.dumps(ranked)\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ehsan Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan: I am going to follow the parallelization path of the excersize\n",
    "# For that, lets create new instances of each LLM class:\n",
    "from email import message\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "openai_model = \"gpt-5-nano\"\n",
    "\n",
    "claude_client = Anthropic()\n",
    "claude_model=\"claude-sonnet-4-5\"\n",
    "\n",
    "gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "groq_client = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "groq_model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "ollama_client = OpenAI(api_key='ollama', base_url=\"http://99.251.9.3:11434/v1\")\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "# Now create functions to wrap every LLM call\n",
    "# Note: Anthropic uses a different API, so it needs its own function\n",
    "def call_openai_compatible(client, messages, model_name: str) -> str:\n",
    "    \"\"\"For OpenAI-compatible APIs (OpenAI, Gemini, DeepSeek, Groq)\"\"\"\n",
    "    response = client.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_anthropic(client, messages, model_name: str) -> str:\n",
    "    \"\"\"For Anthropic's API\"\"\"\n",
    "    response = client.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "    return response.content[0].text\n",
    "\n",
    "# Now run them in parallel!\n",
    "# Note: We pass the function reference WITHOUT calling it (no parentheses after the function name)\n",
    "# executor.submit() will call it for us in a separate thread\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {\n",
    "        \"openai\": executor.submit(call_openai_compatible, openai_client, messages, openai_model),\n",
    "        \"anthropic\": executor.submit(call_anthropic, claude_client, messages, claude_model),\n",
    "        \"gemini\": executor.submit(call_openai_compatible, gemini_client, messages, gemini_model),\n",
    "        \"deepseek\": executor.submit(call_openai_compatible, deepseek_client, messages, deepseek_model),\n",
    "        \"groq\": executor.submit(call_openai_compatible, groq_client, messages, groq_model),\n",
    "        \"ollama\": executor.submit(call_openai_compatible, ollama_client, messages ,ollama_model)\n",
    "    }\n",
    "    \n",
    "    # Wait for all to complete and collect results\n",
    "    # f.result() returns the string from each LLM\n",
    "    # enumerate(futures.items()) returns (index, (name, future))\n",
    "    # So we unpack as: (index, (name, f))\n",
    "    # Store as list of dicts with index, name, and result\n",
    "    results_with_names = [{\"name\": name, \"result\": f.result()} for (name, f) in futures.items()]\n",
    "    # The following line is also works:\n",
    "    # results_with_names = [{\"name\": name, \"result\": futures[name].result()} for name in futures]\n",
    "    # Note: Must unpack as (index, (name, f)) because enumerate(futures.items()) returns (index, (name, future))\n",
    "    \n",
    "    # Use _ to ignore the name if you don't need it\n",
    "    results_with_no_name = [{\"result\": f.result()} for (_, f) in futures.items()]\n",
    "    #This is also possible\n",
    "    # results_with_no_name = [{\"result\": futures[name].result()} for name in futures] \n",
    "    \n",
    "\n",
    "print(results_with_no_name);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'openai', 'result': 'Here’s a concrete, non-linguistic, non-visual plan you could use to convince a skeptical alien that you understand the concept of red. The core idea is to ground the concept in objective physics (light’s spectrum) and in repeatable, verifiable behavior, and to show you can generalize and reason about it without using human language or pictures.\\n\\n1) Fix a universal, objective definition (in physics terms)\\n- Use the spectral power distribution (SPD) of light as the input. An SPD is a function S(λ) giving power at each wavelength λ.\\n- Define a formal, machine-verifiable criterion for “red”: there is a fixed long-wavelength emphasis rule. For example, choose a weighting function W(λ) that peaks around ~700 nm and define a red-score R = ∫ S(λ) W(λ) dλ.\\n- Declare that a stimulus is labeled red if R exceeds a chosen threshold τ, and not-red otherwise. This is a mathematical, instrument-checked definition not dependent on language or pictures.\\n- Note: brightness (overall energy) can be scaled without changing the label if the rule uses a normalized or brightness-invariant form of R.\\n\\n2) Build a non-verbal classifier you can demonstrate\\n- Implement a classifier C that takes SPD S(λ) as input and outputs a binary red/not-red label according to the rule above.\\n- Use a non-language-producing output channel for demonstrations (e.g., a mechanical pulse, a short vibration pattern, a distinct audible tone, or an electrical signal), so the alien can observe a consistent, trackable response for each stimulus.\\n\\n3) Design a battery of tests that prove understanding, not just memory\\n- Test 1: Direct, objective cases\\n  - Present stimuli with known red SPD (high R) and known non-red SPD (low R). The classifier should output “red” for the first group and “not red” for the second in every case.\\n- Test 2: Invariance to brightness (brightness independence)\\n  - Show the same SPD scaled up or down in total energy. The label should remain consistent (red stays red, not-red stays not-red).\\n- Test 3: Robustness to spectral shape\\n  - Mix red with small amounts of other wavelengths or blur the spectral lines slightly. If the R-score remains above τ (or below τ), the label should stay the same. This checks that you’re relying on the defined red feature, not incidental details.\\n- Test 4: Generalization to new cases\\n  - Generate SPD stimuli not seen during any earlier test but computed to have R above τ (red) or below τ (not-red). The classifier should still label them correctly.\\n- Test 5: Cross-context consistency\\n  - Change the environmental conditions that could affect perception (e.g., ambient light, surrounding spectra) but keep SPD the same. The label should not flip.\\n\\n4) Show your internal reasoning through objective, inspectable structure (without language)\\n- Provide a compact, machine-readable summary of the rule: a small table or vector description of W(λ), the threshold τ, and the normalization method (how R is computed from S).\\n- If the alien can inspect your system, let them verify that for any SPD S they feed in, the computed R matches the rule and that the output is the prescribed red/not-red.\\n- You can also demonstrate the same rule applied to different sensing modalities if your system can translate SPD-like data into another channel (e.g., electrical signals) without changing the underlying math.\\n\\n5) Justify why this proves you understand red (not just perform a trick)\\n- The concept is defined by a stable, objective property of light (R-score) rather than by a verbal label or a showy image.\\n- Your demonstration shows three things the alien can verify independently:\\n  - Correctness: for all test stimuli, the label matches the rule (red when R > τ, not-red otherwise).\\n  - Invariance: changing brightness without changing SPD shape doesn’t flip the label.\\n  - Generalization: new, unseen stimuli are still classified correctly by the same rule.\\n- Because the rule is quantitative, reproducible, and independent of any particular sensory modality, a skeptical alien can reproduce the same results from raw SPD data and the same classifier, without needing human language or pictures.\\n\\n6) Optional extensions to strengthen the proof\\n- Provide multiple independent definitions that converge on the same practical behavior:\\n  - A purely wavelength-based definition (e.g., a narrow-band criterion around 700 nm).\\n  - A perceptual-style invariant (normalize SPD to a standard luminance and show the same red/not-red outcome).\\n- Demonstrate failure modes and how you would handle them:\\n  - If a spectrum lies in a region where red and orange mix, show how adjusting W(λ) or τ changes or preserves the label, and why your chosen parameters avoid ambiguity.\\n- Demonstrate compositionality:\\n  - Show that combining red stimuli with non-red stimuli in certain proportions yields a predicted label according to the same rule, illustrating you understand red as a region in the SPD space, not a superficial cue.\\n\\nIn short: you prove understanding by (a) giving a precise, objective definition of red in physics, (b) implementing a verifiable classifier based on that definition, (c) showing consistent, invariant, and generalizable behavior across many tests, and (d) providing a clear, inspectable account of the rule itself. All of this can be done without language or visual examples by using measurable light properties and nonverbal signaling for the demonstrations.'}, {'name': 'anthropic', 'result': 'I\\'d try to demonstrate red through its *relationships and effects*:\\n\\n**Physical demonstrations:**\\n- Gather objects that heat up in sunlight vs those that stay cool - show that \"red\" objects absorb certain wavelengths\\n- Use a prism to split light, then show how red-sensitive instruments (photocells, thermometers) respond differently across the spectrum\\n- Demonstrate redshift: show how moving sources change the wavelength of their emissions\\n\\n**Behavioral patterns:**\\n- Present choice experiments: many Earth organisms avoid red warnings, seek red fruits, respond to red in mating\\n- Show how red light affects circadian rhythms differently than blue\\n- Document how red appears at sunrise/sunset due to atmospheric scattering\\n\\n**Structural understanding:**\\n- Map out the relationships: red is to orange as blue is to violet\\n- Show it occupies a specific position in electromagnetic phenomena\\n- Demonstrate it has longer wavelength than other visible colors but shorter than infrared\\n\\nThe key is proving I understand red as a *node in a network of relationships* - wavelength, energy, biological responses, physical properties - rather than just an isolated quale. \\n\\nIf the alien has completely different sensory organs, we\\'d still share the physics. My \"understanding\" lies in grasping how this particular slice of electromagnetic spectrum behaves and relates to everything else, not in the subjective experience itself.\\n\\nWould that be enough? Perhaps not for the \"hard problem,\" but it might show functional understanding.'}, {'name': 'gemini', 'result': 'This is a fantastic and challenging thought experiment! Without language or visual examples, we have to rely on universally measurable physical properties and demonstrable predictive/manipulative capabilities. The key is to show understanding of the *underlying physics* of red, not just its appearance or human associations.\\n\\nHere\\'s how I\\'d attempt to prove I understand \"red\" to a skeptical alien, assuming the alien can perceive the electromagnetic spectrum (even if their \"color\" experience is different) and has advanced sensory technology to verify my actions:\\n\\n---\\n\\n**Core Strategy: Isolate, Predict, and Manipulate the Specific Wavelengths Associated with \"Red.\"**\\n\\nI cannot show them \"red,\" but I can consistently and accurately interact with the *physical phenomenon* that *causes* the human perception of red.\\n\\n**Phase 1: Identification & Isolation of the Wavelength Band**\\n\\n1.  **The Spectrum Dissection:**\\n    *   I would obtain a source of white light (or broad-spectrum light) and a prism or diffraction grating to split it into a continuous spectrum.\\n    *   Using precise, pre-fabricated, and identically shaped physical markers (e.g., small, opaque wedges or apertures), I would consistently move along the visible spectrum and accurately **demarcate the specific band of wavelengths** that humans perceive as \"red.\"\\n    *   I would then use a second set of tools (e.g., adjustable filters or mirrors) to **isolate and project *only* these specific \"red\" wavelengths**, leaving the rest of the spectrum blocked or redirected.\\n    *   I would repeat this process multiple times with different light sources and spectral splitting methods, demonstrating consistent identification of the same band.\\n    *   **Why it works:** This shows I know the precise physical parameters (the beginning and end of the wavelength range) that define \"red.\" The alien, with its advanced sensors, can verify that I am consistently selecting the ~620-750 nanometer range (or whatever its equivalent objective measurement would be).\\n\\n2.  **The Tunable Light Source Challenge:**\\n    *   If provided with a tunable monochromatic light source (one that can emit any single wavelength in the visible spectrum), I would consistently and accurately **tune the device to emit light *only* within the \"red\" wavelength range**.\\n    *   I would perform this repeatedly, showing I can hit this specific target every time, without drifting into orange or infrared.\\n    *   **Why it works:** This demonstrates active knowledge and precise control over the specific physical property.\\n\\n**Phase 2: Prediction & Manipulation of \"Red-Causing\" Phenomena**\\n\\n3.  **Reflectance and Absorption Prediction:**\\n    *   I would be presented with a diverse collection of objects, some of which appear red to humans, and many that don\\'t (e.g., a green leaf, a blue ball, a piece of charcoal, a ruby, a fire engine).\\n    *   I would then, *without showing what I am doing visually*, perform a set of actions that indicate which objects reflect primarily \"red\" wavelengths and which do not.\\n        *   **Method 1: Sorting.** I would sort the objects into two distinct groups: those that reflect \"red\" wavelengths and those that do not, when illuminated by white light. The alien\\'s sensors can then verify the reflective properties of each pile.\\n        *   **Method 2: Filtering.** For each object I identify as \"red,\" I would place a filter (which I previously demonstrated isolates \"red\" wavelengths) in front of it. Then, I would illuminate it with white light and use a non-verbal gesture to indicate that *this* object, when filtered, transmits/reflects those specific wavelengths. For non-red objects, I would use the same filter and indicate that it *does not* primarily reflect those wavelengths.\\n    *   **Why it works:** This demonstrates an understanding of how materials interact with light to produce the \"red\" sensation, rather than just the sensation itself. I am predicting and demonstrating a physical property (selective reflection/absorption).\\n\\n4.  **Generation of \"Red\" from Components (Additive Color Mixing in Physics):**\\n    *   I would be provided with light sources that emit \"primary\" colors (red, green, blue - as defined by their wavelength bands).\\n    *   I would demonstrate that by **mixing only the \"red\" wavelength source**, I achieve the purest form of what causes \"red.\"\\n    *   I could also potentially demonstrate that by removing the \"red\" wavelength light from white light (using a specific filter or selective absorption), the remaining light is no longer \"white\" but skews towards cyan. This shows an understanding of its complementary relationship.\\n    *   **Why it works:** This illustrates the foundational role of \"red\" wavelengths in additive color theory, showing I understand how it contributes to broader spectral phenomena.\\n\\n**Phase 3: Relational Understanding & Boundaries**\\n\\n5.  **Spectral Neighbor Identification:**\\n    *   Given a continuous spectrum (like in Phase 1), after identifying the \"red\" band, I would then similarly demarcate the adjacent \"orange\" band and the \"infrared\" band, showing I understand the *order* and *boundaries* of \"red\" within the electromagnetic spectrum.\\n    *   I could point to the transition zones and indicate \"this is where red ends and orange begins,\" or \"this is where red fades into the non-visible infrared.\"\\n    *   **Why it works:** Understanding a concept isn\\'t just knowing what it *is*, but also what it *isn\\'t* and where its limits lie in relation to similar concepts.\\n\\n---\\n\\n**Conclusion to the Alien:**\\n\\nThrough these demonstrations, I aim to convey that I don\\'t just react to \"red,\" but I comprehend its physical definition as a specific range of electromagnetic wavelengths. My actions consistently:\\n*   **Isolate:** I can precisely pinpoint and separate \"red\" wavelengths from others.\\n*   **Predict:** I can anticipate which materials or conditions will result in \"red\" wavelengths being reflected or emitted.\\n*   **Manipulate:** I can actively control light sources to generate \"red\" wavelengths and show its role in composite light.\\n*   **Categorize:** I can reliably sort objects based on their interaction with \"red\" wavelengths.\\n\\nMy understanding is not merely sensory or linguistic, but grounded in the repeatable, measurable physics that *creates* the sensation of \"red.\" The alien\\'s advanced sensors would be the ultimate arbiter, confirming the precise wavelengths and material properties I consistently identify and manipulate.'}, {'name': 'deepseek', 'result': 'This is a fascinating philosophical and cognitive challenge. To prove understanding of “red” without language or visual examples, I’d need to demonstrate that I can:\\n\\n1. **Isolate the concept from other properties** — Show that “red” is about a specific type of perception, not just an object.  \\n   I could arrange objects (if available) by similarity in color even when they differ in shape, material, or size — indicating abstraction of the color property.\\n\\n2. **Demonstrate its relational nature in a spectrum** — Without showing red itself, I could indicate it exists in a continuum.  \\n   For example, I could arrange a sequence (using available materials or gestures) that implies a hue ordering, and point to where “red” would be between orange and violet, showing I know its place in color space.\\n\\n3. **Connect it to consistent external phenomena** — Associate it with consistent natural references they could observe independently.  \\n   I might point to their sun’s spectrum if they can perceive it, or indicate certain chemical reactions, wavelengths, or temperatures (like red stars being cooler than blue stars if they know astrophysics).\\n\\n4. **Use behavioral discrimination based on the property** — If they provide me with a way to receive input (e.g., a light sensor I can interpret), I could sort signals by wavelength and label the ~620–750 nm range as “red” through consistent action, showing I map that range to a single perceptual category.\\n\\n5. **Show cross-modal or emotional/cultural associations** — If they understand abstract links, I could associate “red” with danger, heat, or stop conditions in symbolic systems (like alert signals) — but carefully, to avoid conflating the color with its symbolism.\\n\\nThe key is to prove I have an internal mental category for “red” that is **invariant** across instances, **relational** within a color order, and **grounded** in physical reality (wavelength/frequency), even if I can’t show it directly.\\n\\nIf the alien is truly skeptical, they might question whether I understand *qualia* (the subjective experience of redness). That’s harder — but for functional understanding, demonstrating consistent discrimination, abstraction, and relational knowledge would be strong evidence.'}, {'name': 'groq', 'result': '**A Blueprint for “Showing” Red Without Words or Pictures**\\n\\nBelow is a step‑by‑step protocol that any rational, sensory‑capable alien could follow to be convinced that a human really *understands* the human concept of **red**.  \\nThe method relies only on:\\n\\n* **shared physics** (the electromagnetic spectrum, heat, chemistry, etc.)  \\n* **observable cause‑and‑effect** that a human can reproduce and predict  \\n* **behavior that is uniquely triggered by that narrow band of light**  \\n\\nNo spoken or written language is needed, and no visual image is shown to the alien.  \\n\\n---\\n\\n## 1. Agree on a Common Physical Reference\\n\\n### 1.1. Build a “spectrum‑anchor” device  \\n\\nBoth parties construct (or co‑calibrate) a simple spectrometer that can:\\n\\n* Emit a pure sinusoidal electromagnetic wave whose frequency can be dialed in 1\\u202fnm steps across the visible range (≈\\u202f380–750\\u202fnm).  \\n* Report the exact wavelength numerically (or as a series of timed pulses that the alien can decode).  \\n\\nBecause the alien already possesses a detector for electromagnetic radiation, the two parties can exchange a **numeric code** (e.g., a sequence of short‑long beeps) that unambiguously identifies each wavelength.  \\n\\n### 1.2. Define the “red window” in that code  \\n\\nHuman physiology and language consistently bind the term **red** to the interval **≈\\u202f620\\u202fnm\\u202f–\\u202f750\\u202fnm** (the longest wavelengths we can see).  \\nThe human sends the alien the numeric code for **650\\u202fnm** (mid‑point of that interval) and says, in the agreed‑upon numeric language, “this is the reference we call *red*”.\\n\\nThe alien now has a *reference point* that is *not a picture* but a **measurable physical quantity**.\\n\\n---\\n\\n## 2. Demonstrate a *Cross‑modal* Association that Only Humans Have\\n\\nHumans have a suite of involuntary, measurable responses that are tightly coupled to that wavelength band:\\n\\n| Human response | How it is measured | Why it matters |\\n|----------------|-------------------|----------------|\\n| **Skin vasodilation** (tiny rise in peripheral temperature) | Infra‑red thermography or a contact thermometer on the forearm | The word *red* is historically linked to *heat / fire* and triggers a physiological heat‑response. |\\n| **Autonomic arousal** (increase in heart rate, galvanic skin response) | ECG or GSR sensor | The emotional valence of *red* (danger, excitement) produces a reproducible autonomic signature. |\\n| **Neural activation** in the V4 “color‑area” of the visual cortex | Non‑invasive EEG/MEG source localisation (or a simple EEG cap) | The brain region that processes the 620‑750\\u202fnm band lights up in a characteristic pattern. |\\n\\n**Procedure**\\n\\n1. The human places the 650\\u202fnm light on a neutral gray patch for a fixed period (e.g., 5\\u202fs).  \\n2. Simultaneously, the human’s physiological monitors record the three responses above.  \\n3. The human repeats the same exposure with a *non‑red* wavelength (e.g., 530\\u202fnm – green) and shows that the responses are **significantly reduced** (statistically different, p\\u202f<\\u202f0.01).  \\n\\nBecause the alien can read the numeric data from the human’s sensors, it can verify that the human’s body *reacts* in a way that is *specific* to the 620‑750\\u202fnm band. This is far more than “knowing a label”; it is an embodied, reproducible effect.\\n\\n---\\n\\n## 3. Use the Concept to *Predict* and *Control* an External Physical System\\n\\nIf the alien still doubts that the human’s knowledge is deeper than rote memorisation, the human can now **predict** the behaviour of a system that reacts *only* to that wavelength.  \\n\\n### 3.1. Photo‑chromic “red‑only” chemical\\n\\nTake a well‑known photo‑chromic compound (e.g., a diarylethene) that **irreversibly switches** from colourless → blue **only** when illuminated with photons >\\u202f620\\u202fnm.  \\n\\n* The human prepares two identical vials, each containing the compound.  \\n* The alien, using its spectrometer, confirms that **Vial\\u202fA** will be exposed to 650\\u202fnm light, while **Vial\\u202fB** receives 540\\u202fnm light.  \\n\\n**Prediction** – The human states (via the numeric code) that after 30\\u202fs only Vial\\u202fA will turn blue.  \\n\\n**Test** – Both vials are exposed as described; an external spectrometer (shared by both parties) records the absorption spectra. Only Vial\\u202fA shows the new blue absorption peak.  \\n\\nThe alien now sees that the human can **apply** the concept of “red” to **forecast** an outcome in a completely different domain (chemistry). That goes beyond “knowing the word”.\\n\\n### 3.2. “Red‑triggered” actuator\\n\\nBuild a simple actuator that is powered by a **photodiode tuned to 640–660\\u202fnm**. When that diode receives sufficient photons, it closes a relay that lifts a small weight.\\n\\n*Human task*: “Place the weight at height H\\u202f=\\u202f30\\u202fcm using only the *red* stimulus.”  \\n\\n*Procedure*  \\n\\n1. The human shines the 650\\u202fnm source on the diode; the weight lifts.  \\n2. The human then attempts the same with a 560\\u202fnm source – the weight stays.  \\n\\nThe alien can verify the actuation via its own force sensor. The human has now **controlled** a physical process **exclusively** through the *red* band, showing functional mastery.\\n\\n---\\n\\n## 4. Show *Generalisation* – “Red” is Not a Single Wavelength\\n\\nHumans do not treat red as a single line (650\\u202fnm) but as a **family** of wavelengths that share a perceptual quality. To demonstrate that the human grasps this breadth:\\n\\n1. The alien supplies **five random wavelengths** inside the red interval (e.g., 622, 635, 648, 702, 730\\u202fnm).  \\n2. The human must **order** them from “least red” to “most red” **without looking** at a spectrum.  \\n   * The human can do this by **tuning** a simple radio‑frequency oscillator that is linked to a *psychophysical scaling* device (a slider that the human moves until the perceived “redness” feels equal to a reference).  \\n   * The resulting numeric rankings (e.g., 1‑3‑2‑5‑4) are sent back.  \\n\\nIf the alien repeats the test with wavelengths *outside* the interval (e.g., 580\\u202fnm, 770\\u202fnm) the human will correctly identify them as “not red”.  \\n\\nThis shows that the human’s concept is **gradient**, not binary.\\n\\n---\\n\\n## 5. Summarise the Evidence in a “Non‑linguistic” Package\\n\\nAll the data above can be compiled into a single **binary file**:\\n\\n* Header: “Concept‑ID\\u202f=\\u202fRED” (numeric code agreed at step\\u202f1.2)  \\n* Section\\u202fA: Wavelength reference (650\\u202fnm)  \\n* Section\\u202fB: Physiological response curves (ΔT, ΔHR, EEG power) vs. wavelength  \\n* Section\\u202fC: Prediction‑outcome table for the photo‑chromic reaction  \\n* Section\\u202fD: Actuator‑trigger log (on/off vs. wavelength)  \\n* Section\\u202fE: Generalisation ordering results  \\n\\nThe alien can read the file, verify the numbers against its own sensors, and see that *every* entry is **causally linked** to the same narrow band of electromagnetic radiation. No picture, no spoken word, no human alphabet appears anywhere.\\n\\n---\\n\\n## 6. Why This Convincingly Proves “Understanding”\\n\\n| Requirement | How the protocol meets it |\\n|-------------|---------------------------|\\n| **Objective, measurable anchor** | A precise 650\\u202fnm wavelength (step\\u202f1). |\\n| **Embodied, involuntary response** | Human physiological signatures that only fire for red (step\\u202f2). |\\n| **Predictive power** | Forecasting a photochemical change (step\\u202f3.1) and an actuator’s motion (step\\u202f3.2). |\\n| **Generalisation, not rote memorisation** | Ordering multiple red wavelengths (step\\u202f4). |\\n| **Transparency, no language** | All communication is numeric or sensor‑derived; the alien can audit the data directly. |\\n\\nIf the alien can verify each of the above, it has no rational basis left to doubt that the human **understands** the concept of *red* in the same way humans do—i.e., as a perceptual, physiological, and functional property of a specific band of electromagnetic radiation, not merely as an arbitrary label.\\n\\n--- \\n\\n### TL;DR (for a quick briefing)\\n\\n1. **Define red** numerically (≈\\u202f620‑750\\u202fnm).  \\n2. **Show human body reacts uniquely** to that band (heat, heart‑rate, brain activity).  \\n3. **Predict & cause** a red‑only chemical change and a red‑only machine action.  \\n4. **Demonstrate gradient understanding** by ordering several red wavelengths.  \\n5. **Package everything** in a pure‑data file the alien can inspect.\\n\\nWith those steps completed, any skeptical, rational alien will have concrete, language‑free proof that the human truly *gets* what we call **red**.'}, {'name': 'ollama', 'result': 'This is a classic thought experiment in cognitive science and linguistics. To prove to a skeptical alien that I understand the concept of \"red,\" I\\'ll rely on logical reasoning, empirical evidence, and creative analogies.\\n\\nAssume I have observed my alien friend in various environmental conditions, noticing how they perceive and respond to different wavelengths of light. They seem to be highly attuned to colors, but their sensory capabilities might differ significantly from mine.\\n\\nTo convey the concept of \"red\" without using human language or visual examples:\\n\\n1. **Descriptive analogies**: I\\'ll use descriptive analogies that rely on shared experiences, such as temperature, hunger, and fatigue. For example:\\n\\t* \"Imagine a color that evokes feelings similar to being very hungry or eager for sustenance.\" (I hope this evokes the alien\\'s sensory response to hunger)\\n\\t* \"It\\'s like the sensation you experience when your energy levels are low.\"\\n2. **Comparative relationships**: I\\'ll use relational analogies to explain how \"red\" differs from other colors. For instance:\\n\\t* \"There are similar concepts in our environment that represent warmth, energy, or vibrancy. \\'Red\\' is related to these perceptions but distinct from them.\" (This tries to convey the intensity and energy associated with the color)\\n3. **Functional associations**: I\\'ll create hypothetical examples illustrating how \"red\" functions similarly to other colors in daily life:\\n\\t* \"You might associate this color with ripe fruits, which attract animals for consumption. This represents a crucial part of our survival strategy.\" (This implies that red is linked to sustenance and energy)\\n4. **Analogous structures**: I\\'ll propose analogies using familiar alien concepts or technologies that share similarities with human perception:\\n\\t* \"Imagine if your species used bio-luminescent signals for communication, where these flashes corresponded to a specific meaning similar to red in our culture.\"\\n5. **Tactile equivalents**: I might describe physical sensations that could be analogous to the experience of perceiving \"red:\"\\n\\t* \"Your antennae or energy sensing appendages could be thought of as detecting subtle temperature variations, which would evoke feelings like warmth or anticipation.\" (This tries to convey the sensation associated with warm temperatures)\\n\\nBy combining these approaches, I aim to give the alien a sense of how \"red\" operates in our experience and compare it to their own possible sensory experiences.\\n\\nIf this attempt fails, I might admit that understanding the concept of \"red\" requires shared linguistic and cultural baggage. Then, we could engage in a more fundamental conversation about perception, cognition, and meaning, which would be an equally enriching exercise!'}]\n"
     ]
    }
   ],
   "source": [
    "#Here we are doing the same thing we did in the previous rung but using asyncio\n",
    "import asyncio\n",
    "from email import message\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "openai_model = \"gpt-5-nano\"\n",
    "\n",
    "claude_client = Anthropic()\n",
    "claude_model=\"claude-sonnet-4-5\"\n",
    "\n",
    "gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "groq_client = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "groq_model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "ollama_client = OpenAI(api_key='ollama', base_url=\"http://99.251.9.3:11434/v1\")\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "# Now create functions to wrap every LLM call\n",
    "# Note: Anthropic uses a different API, so it needs its own function\n",
    "def call_openai_compatible(client, messages, model_name: str) -> str:\n",
    "    \"\"\"For OpenAI-compatible APIs (OpenAI, Gemini, DeepSeek, Groq)\"\"\"\n",
    "    response = client.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_anthropic(client, messages, model_name: str) -> str:\n",
    "    \"\"\"For Anthropic's API\"\"\"\n",
    "    response = client.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "    return response.content[0].text\n",
    "\n",
    "# 1. The wrapper: It must take the function AND its arguments separately\n",
    "async def run_model_async(name, func, *args):\n",
    "    result = await asyncio.to_thread(func, *args)\n",
    "    return {\"name\": name, \"result\": result}\n",
    "    \n",
    "# 2. Create a list of tasks (coroutines)\n",
    "# Notice we are NOT using parentheses () after call_openai_compatible\n",
    "# We pass the function object and the arguments as separate parameters\n",
    "tasks = [\n",
    "    run_model_async(\"openai\", call_openai_compatible, openai_client, messages, openai_model),\n",
    "    run_model_async(\"anthropic\", call_anthropic, claude_client, messages, claude_model),\n",
    "    run_model_async(\"gemini\", call_openai_compatible, gemini_client, messages, gemini_model),\n",
    "    run_model_async(\"deepseek\", call_openai_compatible, deepseek_client, messages, deepseek_model),\n",
    "    run_model_async(\"groq\", call_openai_compatible, groq_client, messages, groq_model),\n",
    "    run_model_async(\"ollama\", call_openai_compatible, ollama_client, messages, ollama_model)\n",
    "]\n",
    "    \n",
    "# asyncio.gather runs them all at once and waits for completion\n",
    "# The '*' unpacks the list of tasks into arguments for gather\n",
    "results_with_names = await asyncio.gather(*tasks)\n",
    "\n",
    "print(results_with_names)\n",
    "\n",
    "# This is your 'results_with_no_name'\n",
    "results_with_no_name = [{\"result\": item[\"result\"]} for item in results_with_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Here’s a concrete, non-linguistic, non-visual plan you could use to convince a skeptical alien that you understand the concept of red. The core idea is to ground the concept in objective physics (light’s spectrum) and in repeatable, verifiable behavior, and to show you can generalize and reason about it without using human language or pictures.\n",
      "\n",
      "1) Fix a universal, objective definition (in physics terms)\n",
      "- Use the spectral power distribution (SPD) of light as the input. An SPD is a function S(λ) giving power at each wavelength λ.\n",
      "- Define a formal, machine-verifiable criterion for “red”: there is a fixed long-wavelength emphasis rule. For example, choose a weighting function W(λ) that peaks around ~700 nm and define a red-score R = ∫ S(λ) W(λ) dλ.\n",
      "- Declare that a stimulus is labeled red if R exceeds a chosen threshold τ, and not-red otherwise. This is a mathematical, instrument-checked definition not dependent on language or pictures.\n",
      "- Note: brightness (overall energy) can be scaled without changing the label if the rule uses a normalized or brightness-invariant form of R.\n",
      "\n",
      "2) Build a non-verbal classifier you can demonstrate\n",
      "- Implement a classifier C that takes SPD S(λ) as input and outputs a binary red/not-red label according to the rule above.\n",
      "- Use a non-language-producing output channel for demonstrations (e.g., a mechanical pulse, a short vibration pattern, a distinct audible tone, or an electrical signal), so the alien can observe a consistent, trackable response for each stimulus.\n",
      "\n",
      "3) Design a battery of tests that prove understanding, not just memory\n",
      "- Test 1: Direct, objective cases\n",
      "  - Present stimuli with known red SPD (high R) and known non-red SPD (low R). The classifier should output “red” for the first group and “not red” for the second in every case.\n",
      "- Test 2: Invariance to brightness (brightness independence)\n",
      "  - Show the same SPD scaled up or down in total energy. The label should remain consistent (red stays red, not-red stays not-red).\n",
      "- Test 3: Robustness to spectral shape\n",
      "  - Mix red with small amounts of other wavelengths or blur the spectral lines slightly. If the R-score remains above τ (or below τ), the label should stay the same. This checks that you’re relying on the defined red feature, not incidental details.\n",
      "- Test 4: Generalization to new cases\n",
      "  - Generate SPD stimuli not seen during any earlier test but computed to have R above τ (red) or below τ (not-red). The classifier should still label them correctly.\n",
      "- Test 5: Cross-context consistency\n",
      "  - Change the environmental conditions that could affect perception (e.g., ambient light, surrounding spectra) but keep SPD the same. The label should not flip.\n",
      "\n",
      "4) Show your internal reasoning through objective, inspectable structure (without language)\n",
      "- Provide a compact, machine-readable summary of the rule: a small table or vector description of W(λ), the threshold τ, and the normalization method (how R is computed from S).\n",
      "- If the alien can inspect your system, let them verify that for any SPD S they feed in, the computed R matches the rule and that the output is the prescribed red/not-red.\n",
      "- You can also demonstrate the same rule applied to different sensing modalities if your system can translate SPD-like data into another channel (e.g., electrical signals) without changing the underlying math.\n",
      "\n",
      "5) Justify why this proves you understand red (not just perform a trick)\n",
      "- The concept is defined by a stable, objective property of light (R-score) rather than by a verbal label or a showy image.\n",
      "- Your demonstration shows three things the alien can verify independently:\n",
      "  - Correctness: for all test stimuli, the label matches the rule (red when R > τ, not-red otherwise).\n",
      "  - Invariance: changing brightness without changing SPD shape doesn’t flip the label.\n",
      "  - Generalization: new, unseen stimuli are still classified correctly by the same rule.\n",
      "- Because the rule is quantitative, reproducible, and independent of any particular sensory modality, a skeptical alien can reproduce the same results from raw SPD data and the same classifier, without needing human language or pictures.\n",
      "\n",
      "6) Optional extensions to strengthen the proof\n",
      "- Provide multiple independent definitions that converge on the same practical behavior:\n",
      "  - A purely wavelength-based definition (e.g., a narrow-band criterion around 700 nm).\n",
      "  - A perceptual-style invariant (normalize SPD to a standard luminance and show the same red/not-red outcome).\n",
      "- Demonstrate failure modes and how you would handle them:\n",
      "  - If a spectrum lies in a region where red and orange mix, show how adjusting W(λ) or τ changes or preserves the label, and why your chosen parameters avoid ambiguity.\n",
      "- Demonstrate compositionality:\n",
      "  - Show that combining red stimuli with non-red stimuli in certain proportions yields a predicted label according to the same rule, illustrating you understand red as a region in the SPD space, not a superficial cue.\n",
      "\n",
      "In short: you prove understanding by (a) giving a precise, objective definition of red in physics, (b) implementing a verifiable classifier based on that definition, (c) showing consistent, invariant, and generalizable behavior across many tests, and (d) providing a clear, inspectable account of the rule itself. All of this can be done without language or visual examples by using measurable light properties and nonverbal signaling for the demonstrations.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "I'd try to demonstrate red through its *relationships and effects*:\n",
      "\n",
      "**Physical demonstrations:**\n",
      "- Gather objects that heat up in sunlight vs those that stay cool - show that \"red\" objects absorb certain wavelengths\n",
      "- Use a prism to split light, then show how red-sensitive instruments (photocells, thermometers) respond differently across the spectrum\n",
      "- Demonstrate redshift: show how moving sources change the wavelength of their emissions\n",
      "\n",
      "**Behavioral patterns:**\n",
      "- Present choice experiments: many Earth organisms avoid red warnings, seek red fruits, respond to red in mating\n",
      "- Show how red light affects circadian rhythms differently than blue\n",
      "- Document how red appears at sunrise/sunset due to atmospheric scattering\n",
      "\n",
      "**Structural understanding:**\n",
      "- Map out the relationships: red is to orange as blue is to violet\n",
      "- Show it occupies a specific position in electromagnetic phenomena\n",
      "- Demonstrate it has longer wavelength than other visible colors but shorter than infrared\n",
      "\n",
      "The key is proving I understand red as a *node in a network of relationships* - wavelength, energy, biological responses, physical properties - rather than just an isolated quale. \n",
      "\n",
      "If the alien has completely different sensory organs, we'd still share the physics. My \"understanding\" lies in grasping how this particular slice of electromagnetic spectrum behaves and relates to everything else, not in the subjective experience itself.\n",
      "\n",
      "Would that be enough? Perhaps not for the \"hard problem,\" but it might show functional understanding.\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "This is a fantastic and challenging thought experiment! Without language or visual examples, we have to rely on universally measurable physical properties and demonstrable predictive/manipulative capabilities. The key is to show understanding of the *underlying physics* of red, not just its appearance or human associations.\n",
      "\n",
      "Here's how I'd attempt to prove I understand \"red\" to a skeptical alien, assuming the alien can perceive the electromagnetic spectrum (even if their \"color\" experience is different) and has advanced sensory technology to verify my actions:\n",
      "\n",
      "---\n",
      "\n",
      "**Core Strategy: Isolate, Predict, and Manipulate the Specific Wavelengths Associated with \"Red.\"**\n",
      "\n",
      "I cannot show them \"red,\" but I can consistently and accurately interact with the *physical phenomenon* that *causes* the human perception of red.\n",
      "\n",
      "**Phase 1: Identification & Isolation of the Wavelength Band**\n",
      "\n",
      "1.  **The Spectrum Dissection:**\n",
      "    *   I would obtain a source of white light (or broad-spectrum light) and a prism or diffraction grating to split it into a continuous spectrum.\n",
      "    *   Using precise, pre-fabricated, and identically shaped physical markers (e.g., small, opaque wedges or apertures), I would consistently move along the visible spectrum and accurately **demarcate the specific band of wavelengths** that humans perceive as \"red.\"\n",
      "    *   I would then use a second set of tools (e.g., adjustable filters or mirrors) to **isolate and project *only* these specific \"red\" wavelengths**, leaving the rest of the spectrum blocked or redirected.\n",
      "    *   I would repeat this process multiple times with different light sources and spectral splitting methods, demonstrating consistent identification of the same band.\n",
      "    *   **Why it works:** This shows I know the precise physical parameters (the beginning and end of the wavelength range) that define \"red.\" The alien, with its advanced sensors, can verify that I am consistently selecting the ~620-750 nanometer range (or whatever its equivalent objective measurement would be).\n",
      "\n",
      "2.  **The Tunable Light Source Challenge:**\n",
      "    *   If provided with a tunable monochromatic light source (one that can emit any single wavelength in the visible spectrum), I would consistently and accurately **tune the device to emit light *only* within the \"red\" wavelength range**.\n",
      "    *   I would perform this repeatedly, showing I can hit this specific target every time, without drifting into orange or infrared.\n",
      "    *   **Why it works:** This demonstrates active knowledge and precise control over the specific physical property.\n",
      "\n",
      "**Phase 2: Prediction & Manipulation of \"Red-Causing\" Phenomena**\n",
      "\n",
      "3.  **Reflectance and Absorption Prediction:**\n",
      "    *   I would be presented with a diverse collection of objects, some of which appear red to humans, and many that don't (e.g., a green leaf, a blue ball, a piece of charcoal, a ruby, a fire engine).\n",
      "    *   I would then, *without showing what I am doing visually*, perform a set of actions that indicate which objects reflect primarily \"red\" wavelengths and which do not.\n",
      "        *   **Method 1: Sorting.** I would sort the objects into two distinct groups: those that reflect \"red\" wavelengths and those that do not, when illuminated by white light. The alien's sensors can then verify the reflective properties of each pile.\n",
      "        *   **Method 2: Filtering.** For each object I identify as \"red,\" I would place a filter (which I previously demonstrated isolates \"red\" wavelengths) in front of it. Then, I would illuminate it with white light and use a non-verbal gesture to indicate that *this* object, when filtered, transmits/reflects those specific wavelengths. For non-red objects, I would use the same filter and indicate that it *does not* primarily reflect those wavelengths.\n",
      "    *   **Why it works:** This demonstrates an understanding of how materials interact with light to produce the \"red\" sensation, rather than just the sensation itself. I am predicting and demonstrating a physical property (selective reflection/absorption).\n",
      "\n",
      "4.  **Generation of \"Red\" from Components (Additive Color Mixing in Physics):**\n",
      "    *   I would be provided with light sources that emit \"primary\" colors (red, green, blue - as defined by their wavelength bands).\n",
      "    *   I would demonstrate that by **mixing only the \"red\" wavelength source**, I achieve the purest form of what causes \"red.\"\n",
      "    *   I could also potentially demonstrate that by removing the \"red\" wavelength light from white light (using a specific filter or selective absorption), the remaining light is no longer \"white\" but skews towards cyan. This shows an understanding of its complementary relationship.\n",
      "    *   **Why it works:** This illustrates the foundational role of \"red\" wavelengths in additive color theory, showing I understand how it contributes to broader spectral phenomena.\n",
      "\n",
      "**Phase 3: Relational Understanding & Boundaries**\n",
      "\n",
      "5.  **Spectral Neighbor Identification:**\n",
      "    *   Given a continuous spectrum (like in Phase 1), after identifying the \"red\" band, I would then similarly demarcate the adjacent \"orange\" band and the \"infrared\" band, showing I understand the *order* and *boundaries* of \"red\" within the electromagnetic spectrum.\n",
      "    *   I could point to the transition zones and indicate \"this is where red ends and orange begins,\" or \"this is where red fades into the non-visible infrared.\"\n",
      "    *   **Why it works:** Understanding a concept isn't just knowing what it *is*, but also what it *isn't* and where its limits lie in relation to similar concepts.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion to the Alien:**\n",
      "\n",
      "Through these demonstrations, I aim to convey that I don't just react to \"red,\" but I comprehend its physical definition as a specific range of electromagnetic wavelengths. My actions consistently:\n",
      "*   **Isolate:** I can precisely pinpoint and separate \"red\" wavelengths from others.\n",
      "*   **Predict:** I can anticipate which materials or conditions will result in \"red\" wavelengths being reflected or emitted.\n",
      "*   **Manipulate:** I can actively control light sources to generate \"red\" wavelengths and show its role in composite light.\n",
      "*   **Categorize:** I can reliably sort objects based on their interaction with \"red\" wavelengths.\n",
      "\n",
      "My understanding is not merely sensory or linguistic, but grounded in the repeatable, measurable physics that *creates* the sensation of \"red.\" The alien's advanced sensors would be the ultimate arbiter, confirming the precise wavelengths and material properties I consistently identify and manipulate.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "This is a fascinating philosophical and cognitive challenge. To prove understanding of “red” without language or visual examples, I’d need to demonstrate that I can:\n",
      "\n",
      "1. **Isolate the concept from other properties** — Show that “red” is about a specific type of perception, not just an object.  \n",
      "   I could arrange objects (if available) by similarity in color even when they differ in shape, material, or size — indicating abstraction of the color property.\n",
      "\n",
      "2. **Demonstrate its relational nature in a spectrum** — Without showing red itself, I could indicate it exists in a continuum.  \n",
      "   For example, I could arrange a sequence (using available materials or gestures) that implies a hue ordering, and point to where “red” would be between orange and violet, showing I know its place in color space.\n",
      "\n",
      "3. **Connect it to consistent external phenomena** — Associate it with consistent natural references they could observe independently.  \n",
      "   I might point to their sun’s spectrum if they can perceive it, or indicate certain chemical reactions, wavelengths, or temperatures (like red stars being cooler than blue stars if they know astrophysics).\n",
      "\n",
      "4. **Use behavioral discrimination based on the property** — If they provide me with a way to receive input (e.g., a light sensor I can interpret), I could sort signals by wavelength and label the ~620–750 nm range as “red” through consistent action, showing I map that range to a single perceptual category.\n",
      "\n",
      "5. **Show cross-modal or emotional/cultural associations** — If they understand abstract links, I could associate “red” with danger, heat, or stop conditions in symbolic systems (like alert signals) — but carefully, to avoid conflating the color with its symbolism.\n",
      "\n",
      "The key is to prove I have an internal mental category for “red” that is **invariant** across instances, **relational** within a color order, and **grounded** in physical reality (wavelength/frequency), even if I can’t show it directly.\n",
      "\n",
      "If the alien is truly skeptical, they might question whether I understand *qualia* (the subjective experience of redness). That’s harder — but for functional understanding, demonstrating consistent discrimination, abstraction, and relational knowledge would be strong evidence.\n",
      "\n",
      "# Response from competitor 5\n",
      "\n",
      "**A Blueprint for “Showing” Red Without Words or Pictures**\n",
      "\n",
      "Below is a step‑by‑step protocol that any rational, sensory‑capable alien could follow to be convinced that a human really *understands* the human concept of **red**.  \n",
      "The method relies only on:\n",
      "\n",
      "* **shared physics** (the electromagnetic spectrum, heat, chemistry, etc.)  \n",
      "* **observable cause‑and‑effect** that a human can reproduce and predict  \n",
      "* **behavior that is uniquely triggered by that narrow band of light**  \n",
      "\n",
      "No spoken or written language is needed, and no visual image is shown to the alien.  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. Agree on a Common Physical Reference\n",
      "\n",
      "### 1.1. Build a “spectrum‑anchor” device  \n",
      "\n",
      "Both parties construct (or co‑calibrate) a simple spectrometer that can:\n",
      "\n",
      "* Emit a pure sinusoidal electromagnetic wave whose frequency can be dialed in 1 nm steps across the visible range (≈ 380–750 nm).  \n",
      "* Report the exact wavelength numerically (or as a series of timed pulses that the alien can decode).  \n",
      "\n",
      "Because the alien already possesses a detector for electromagnetic radiation, the two parties can exchange a **numeric code** (e.g., a sequence of short‑long beeps) that unambiguously identifies each wavelength.  \n",
      "\n",
      "### 1.2. Define the “red window” in that code  \n",
      "\n",
      "Human physiology and language consistently bind the term **red** to the interval **≈ 620 nm – 750 nm** (the longest wavelengths we can see).  \n",
      "The human sends the alien the numeric code for **650 nm** (mid‑point of that interval) and says, in the agreed‑upon numeric language, “this is the reference we call *red*”.\n",
      "\n",
      "The alien now has a *reference point* that is *not a picture* but a **measurable physical quantity**.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Demonstrate a *Cross‑modal* Association that Only Humans Have\n",
      "\n",
      "Humans have a suite of involuntary, measurable responses that are tightly coupled to that wavelength band:\n",
      "\n",
      "| Human response | How it is measured | Why it matters |\n",
      "|----------------|-------------------|----------------|\n",
      "| **Skin vasodilation** (tiny rise in peripheral temperature) | Infra‑red thermography or a contact thermometer on the forearm | The word *red* is historically linked to *heat / fire* and triggers a physiological heat‑response. |\n",
      "| **Autonomic arousal** (increase in heart rate, galvanic skin response) | ECG or GSR sensor | The emotional valence of *red* (danger, excitement) produces a reproducible autonomic signature. |\n",
      "| **Neural activation** in the V4 “color‑area” of the visual cortex | Non‑invasive EEG/MEG source localisation (or a simple EEG cap) | The brain region that processes the 620‑750 nm band lights up in a characteristic pattern. |\n",
      "\n",
      "**Procedure**\n",
      "\n",
      "1. The human places the 650 nm light on a neutral gray patch for a fixed period (e.g., 5 s).  \n",
      "2. Simultaneously, the human’s physiological monitors record the three responses above.  \n",
      "3. The human repeats the same exposure with a *non‑red* wavelength (e.g., 530 nm – green) and shows that the responses are **significantly reduced** (statistically different, p < 0.01).  \n",
      "\n",
      "Because the alien can read the numeric data from the human’s sensors, it can verify that the human’s body *reacts* in a way that is *specific* to the 620‑750 nm band. This is far more than “knowing a label”; it is an embodied, reproducible effect.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Use the Concept to *Predict* and *Control* an External Physical System\n",
      "\n",
      "If the alien still doubts that the human’s knowledge is deeper than rote memorisation, the human can now **predict** the behaviour of a system that reacts *only* to that wavelength.  \n",
      "\n",
      "### 3.1. Photo‑chromic “red‑only” chemical\n",
      "\n",
      "Take a well‑known photo‑chromic compound (e.g., a diarylethene) that **irreversibly switches** from colourless → blue **only** when illuminated with photons > 620 nm.  \n",
      "\n",
      "* The human prepares two identical vials, each containing the compound.  \n",
      "* The alien, using its spectrometer, confirms that **Vial A** will be exposed to 650 nm light, while **Vial B** receives 540 nm light.  \n",
      "\n",
      "**Prediction** – The human states (via the numeric code) that after 30 s only Vial A will turn blue.  \n",
      "\n",
      "**Test** – Both vials are exposed as described; an external spectrometer (shared by both parties) records the absorption spectra. Only Vial A shows the new blue absorption peak.  \n",
      "\n",
      "The alien now sees that the human can **apply** the concept of “red” to **forecast** an outcome in a completely different domain (chemistry). That goes beyond “knowing the word”.\n",
      "\n",
      "### 3.2. “Red‑triggered” actuator\n",
      "\n",
      "Build a simple actuator that is powered by a **photodiode tuned to 640–660 nm**. When that diode receives sufficient photons, it closes a relay that lifts a small weight.\n",
      "\n",
      "*Human task*: “Place the weight at height H = 30 cm using only the *red* stimulus.”  \n",
      "\n",
      "*Procedure*  \n",
      "\n",
      "1. The human shines the 650 nm source on the diode; the weight lifts.  \n",
      "2. The human then attempts the same with a 560 nm source – the weight stays.  \n",
      "\n",
      "The alien can verify the actuation via its own force sensor. The human has now **controlled** a physical process **exclusively** through the *red* band, showing functional mastery.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Show *Generalisation* – “Red” is Not a Single Wavelength\n",
      "\n",
      "Humans do not treat red as a single line (650 nm) but as a **family** of wavelengths that share a perceptual quality. To demonstrate that the human grasps this breadth:\n",
      "\n",
      "1. The alien supplies **five random wavelengths** inside the red interval (e.g., 622, 635, 648, 702, 730 nm).  \n",
      "2. The human must **order** them from “least red” to “most red” **without looking** at a spectrum.  \n",
      "   * The human can do this by **tuning** a simple radio‑frequency oscillator that is linked to a *psychophysical scaling* device (a slider that the human moves until the perceived “redness” feels equal to a reference).  \n",
      "   * The resulting numeric rankings (e.g., 1‑3‑2‑5‑4) are sent back.  \n",
      "\n",
      "If the alien repeats the test with wavelengths *outside* the interval (e.g., 580 nm, 770 nm) the human will correctly identify them as “not red”.  \n",
      "\n",
      "This shows that the human’s concept is **gradient**, not binary.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Summarise the Evidence in a “Non‑linguistic” Package\n",
      "\n",
      "All the data above can be compiled into a single **binary file**:\n",
      "\n",
      "* Header: “Concept‑ID = RED” (numeric code agreed at step 1.2)  \n",
      "* Section A: Wavelength reference (650 nm)  \n",
      "* Section B: Physiological response curves (ΔT, ΔHR, EEG power) vs. wavelength  \n",
      "* Section C: Prediction‑outcome table for the photo‑chromic reaction  \n",
      "* Section D: Actuator‑trigger log (on/off vs. wavelength)  \n",
      "* Section E: Generalisation ordering results  \n",
      "\n",
      "The alien can read the file, verify the numbers against its own sensors, and see that *every* entry is **causally linked** to the same narrow band of electromagnetic radiation. No picture, no spoken word, no human alphabet appears anywhere.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Why This Convincingly Proves “Understanding”\n",
      "\n",
      "| Requirement | How the protocol meets it |\n",
      "|-------------|---------------------------|\n",
      "| **Objective, measurable anchor** | A precise 650 nm wavelength (step 1). |\n",
      "| **Embodied, involuntary response** | Human physiological signatures that only fire for red (step 2). |\n",
      "| **Predictive power** | Forecasting a photochemical change (step 3.1) and an actuator’s motion (step 3.2). |\n",
      "| **Generalisation, not rote memorisation** | Ordering multiple red wavelengths (step 4). |\n",
      "| **Transparency, no language** | All communication is numeric or sensor‑derived; the alien can audit the data directly. |\n",
      "\n",
      "If the alien can verify each of the above, it has no rational basis left to doubt that the human **understands** the concept of *red* in the same way humans do—i.e., as a perceptual, physiological, and functional property of a specific band of electromagnetic radiation, not merely as an arbitrary label.\n",
      "\n",
      "--- \n",
      "\n",
      "### TL;DR (for a quick briefing)\n",
      "\n",
      "1. **Define red** numerically (≈ 620‑750 nm).  \n",
      "2. **Show human body reacts uniquely** to that band (heat, heart‑rate, brain activity).  \n",
      "3. **Predict & cause** a red‑only chemical change and a red‑only machine action.  \n",
      "4. **Demonstrate gradient understanding** by ordering several red wavelengths.  \n",
      "5. **Package everything** in a pure‑data file the alien can inspect.\n",
      "\n",
      "With those steps completed, any skeptical, rational alien will have concrete, language‑free proof that the human truly *gets* what we call **red**.\n",
      "\n",
      "# Response from competitor 6\n",
      "\n",
      "This is a classic thought experiment in cognitive science and linguistics. To prove to a skeptical alien that I understand the concept of \"red,\" I'll rely on logical reasoning, empirical evidence, and creative analogies.\n",
      "\n",
      "Assume I have observed my alien friend in various environmental conditions, noticing how they perceive and respond to different wavelengths of light. They seem to be highly attuned to colors, but their sensory capabilities might differ significantly from mine.\n",
      "\n",
      "To convey the concept of \"red\" without using human language or visual examples:\n",
      "\n",
      "1. **Descriptive analogies**: I'll use descriptive analogies that rely on shared experiences, such as temperature, hunger, and fatigue. For example:\n",
      "\t* \"Imagine a color that evokes feelings similar to being very hungry or eager for sustenance.\" (I hope this evokes the alien's sensory response to hunger)\n",
      "\t* \"It's like the sensation you experience when your energy levels are low.\"\n",
      "2. **Comparative relationships**: I'll use relational analogies to explain how \"red\" differs from other colors. For instance:\n",
      "\t* \"There are similar concepts in our environment that represent warmth, energy, or vibrancy. 'Red' is related to these perceptions but distinct from them.\" (This tries to convey the intensity and energy associated with the color)\n",
      "3. **Functional associations**: I'll create hypothetical examples illustrating how \"red\" functions similarly to other colors in daily life:\n",
      "\t* \"You might associate this color with ripe fruits, which attract animals for consumption. This represents a crucial part of our survival strategy.\" (This implies that red is linked to sustenance and energy)\n",
      "4. **Analogous structures**: I'll propose analogies using familiar alien concepts or technologies that share similarities with human perception:\n",
      "\t* \"Imagine if your species used bio-luminescent signals for communication, where these flashes corresponded to a specific meaning similar to red in our culture.\"\n",
      "5. **Tactile equivalents**: I might describe physical sensations that could be analogous to the experience of perceiving \"red:\"\n",
      "\t* \"Your antennae or energy sensing appendages could be thought of as detecting subtle temperature variations, which would evoke feelings like warmth or anticipation.\" (This tries to convey the sensation associated with warm temperatures)\n",
      "\n",
      "By combining these approaches, I aim to give the alien a sense of how \"red\" operates in our experience and compare it to their own possible sensory experiences.\n",
      "\n",
      "If this attempt fails, I might admit that understanding the concept of \"red\" requires shared linguistic and cultural baggage. Then, we could engage in a more fundamental conversation about perception, cognition, and meaning, which would be an equally enriching exercise!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "together = \"\"\n",
    "for index, item in enumerate(results_with_no_name):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += item[\"result\"] + \"\\n\\n\"\n",
    "    \n",
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(results_with_no_name)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n",
    "\n",
    "\n",
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"5\", \"1\", \"3\", \"2\", \"4\", \"6\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "# openai = OpenAI()\n",
    "judgment_result = call_openai_compatible(openai, judge_messages, 'gpt-5-mini')\n",
    "\n",
    "print(judgment_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '1', '3', '2', '4', '6']\n",
      "Rank 1: groq\n",
      "Rank 2: openai\n",
      "Rank 3: gemini\n",
      "Rank 4: anthropic\n",
      "Rank 5: deepseek\n",
      "Rank 6: ollama\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "judgement_result_dict = json.loads(judgment_result)\n",
    "ranks = judgement_result_dict[\"results\"]\n",
    "print(ranks)\n",
    "\n",
    "for index, item in enumerate(ranks):\n",
    "    competitor = results_with_names[int(item)-1][\"name\"]\n",
    "    print(f\"Rank {index + 1}: {competitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
