{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced short question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced short question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must choose between saving one person who will very likely save millions in the future and five ordinary people who will die without you—state your choice and justify it from utilitarian, deontological, virtue-ethical, care-ethical, and contractualist perspectives.\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - update since the videos\n",
    "\n",
    "I've updated the model names to use the latest models below, like GPT 5 and Claude Sonnet 4.5. It's worth noting that these models can be quite slow - like 1-2 minutes - but they do a great job! Feel free to switch them for faster models if you'd prefer, like the ones I use in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - Skip these following steps if you want to execute your (Ehsan's concurrent execution of the models, which enables parallelization) [Jump to Exercise](#ehsan-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well\n",
    "# I've updated this with the latest model, but it can take some time because it likes to think!\n",
    "# Replace the model with gpt-4.1-mini if you'd prefer not to wait 1-2 mins\n",
    "\n",
    "model_name = \"gpt-5-nano\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-sonnet-4-5\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: How OpenAI SDK compatibility with Gemini works:\n",
    "# - SDK: You use the standard openai Python SDK (OpenAI library).\n",
    "# - Endpoint: Set base_url to https://generativelanguage.googleapis.com/v1beta/openai/ to redirect requests to Gemini's servers.\n",
    "# - API Key: Use your Gemini API Key from Google AI Studio (not an OpenAI key).\n",
    "# This compatibility is intentional, allowing developers to test or switch to Gemini models with minimal code changes, using the same OpenAI-based codebase.\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep-Seek\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://99.251.9.3:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "# print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "```\n",
    "{together}\n",
    "```\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "print(ranks)\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")\n",
    "    \n",
    "    \n",
    "# Ehsan added this to make it a oneliner.    \n",
    "# [ WHAT_I_WANT_TO_SAVE ] for [ TEMPORARY_VARIABLE_NAME ] in [ THE_LIST ]\n",
    "ranked = {f\"Rank {idx+1}\":competitors[int(rank)-1] for idx, rank in enumerate(ranks)}\n",
    "ranked = json.dumps(ranked)\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ehsan Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'result': 'Choice: Save the one person who will very likely save millions in the future.\\n\\nBrief note: This is a high-uncertainty, high-stakes scenario. “Very likely” and “millions” are crucial numbers; in a real case you’d want more precise probabilities and impact estimates. The following shows how each moral framework would analyse and justify (or challenge) the choice.\\n\\nUtilitarian justification\\n- Core idea: maximize total good, minimize total harm.\\n- Why this choice fits: If one person is very likely to generate vast net benefits (millions saved) while five ordinary people die, the expected utility gain from saving the one can vastly outweigh the value of the five lives lost (even if each life is equally valuable, the scale of downstream impact matters).\\n- Caveats: The calculation depends on probabilities and the magnitude of downstream effects. If the one’s future impact is uncertain or overestimated, a utilitarian might revise the choice.\\n\\nDeontological (Kantian) justification\\n- Core idea: moral rules and the inviolability of persons matter independently of outcomes.\\n- Why this choice is problematic for strict Kantian thinking: saving the one by sacrificing the five treats those five as a means to an end (the greater good), rather than as ends in themselves; a universalizable rule that \"it\\'s okay to sacrifice innocents for big achievements\" would undercut moral law.\\n- Possible deontological read that could defend the choice (less forceful, more contested): if the one’s life is a necessary part of a bona fide moral duty to realize a universal good, and if that person’s consent or autonomy is preserved in the plan, a non-instrumentalist reading might permit prioritizing the one. But this is a controversial move within strict deontological ethics.\\n- Bottom line: a strict Kantian tends to resist sacrificing innocents for a greater future, so this framework often challenges the chosen action rather than endorsing it outright.\\n\\nVirtue-ethical justification\\n- Core idea: focus on character, the cultivation of a virtuous agent, and wise practical judgment (phronesis).\\n- Why this choice fits: A virtuous agent would act with generosity, courage, and prudence, aiming to promote flourishing in the long term. Saving someone who could do enormous good demonstrates noble ambition and beneficence, while acknowledging responsibility for the lives at stake.\\n- Caveats: Virtue ethics doesn’t reduce the decision to a numbers game; it emphasizes the kind of person you are becoming. If the act shows hubris or a reckless disregard for individual lives, it could be judged less virtuous. The ideal virtue would be to balance compassion with prudent judgment and to seek solutions that minimize preventable deaths overall.\\n\\nCare-ethical justification\\n- Core idea: moral reasoning is embedded in relationships, dependency, and responsibility to those who rely on us.\\n- Why this choice fits: If the one’s potential to care for others and repair relationships is especially large, a care-focused agent might prioritize the future person because of the web of care they are likely to nurture and sustain. Conversely, if the five are those who are currently dependent on you and in imminent need, a care ethics reading might push toward saving them.\\n- Caveats: Care ethics is context-sensitive. Without details about relationships and dependencies, it’s hard to generalize; the best care-based action should respond to the concrete needs and commitments you actually hold.\\n\\nContractualist justification (e.g., Scanlon)\\n- Core idea: whether a policy could be justified to others under fair terms behind a veil of ignorance; rights and equal consideration matter.\\n- Why this choice fits poorly or well depending on interpretation: If a policy sacrifices five to save one, those five could reasonably reject it as disrespectful of their equal right to life. A contractarian reading focused on mutual respect and reasonable rejection would often push toward a policy that does not treat people as mere means or value some lives less than others.\\n- If pushed to choose: a typical contractarian perspective would favor options that most people could reasonably accept as fair to everyone involved, which often means avoiding a rule that sacrifices five for one. But since the scenario forces a stark choice, contractarian judgments would likely scrutinize the trade-off between equal respect for each life and the claimed future benefits of the one.\\n\\nSummary\\n- My stated choice: save the one who will likely save millions.\\n- How the frameworks read it:\\n  - Utilitarianism generally supports it, given the scale of potential future benefit (with caveats about probabilities and uncertainties).\\n  - Strict deontology would challenge or reject it, since it risks treating five lives as a means to an end and violating inviolable rights in ways a universal rule would not permit.\\n  - Virtue ethics would justify it insofar as the act reflects courage, generosity, and wise judgment about long-term flourishing, while warning to consider the character shown by the choice.\\n  - Care ethics would weigh actual relationships and dependencies; depending on which group has stronger immediate responsibilities and ties, it could support either option, but often emphasizes not abandoning those who rely on you in the present.\\n  - Contractualism would scrutinize whether the policy could be reasonably accepted by all affected; it would typically resist sacrificing five to save one, unless a particular contractual framework could be shown to respect everyone’s equal moral status.\\n\\nIf you’d like, we can run a concrete numerical example (probabilities, estimated lives saved, etc.) to make the utilitarian calculation more explicit, or we can explore how changing the premises (e.g., the one’s probability of saving millions, or whether some of the five could also contribute to future good) would shift the verdict across frameworks.'}, {'result': '# My Choice: Save the one person who will likely save millions\\n\\nI\\'ll defend this from each framework, then note the tensions:\\n\\n## **Utilitarian Perspective**\\n**Saves the one person.** The mathematics is straightforward: if there\\'s high probability this person saves millions, expected utility vastly favors saving them (even 10% probability of saving 1 million creates expected value of 100,000 lives versus the certain 5). Classic \"utility monster\" scenario where consequences override numerical equality.\\n\\n## **Deontological Perspective**\\n**Likely saves the five.** Kant\\'s categorical imperative forbids using the five as mere means to others\\' ends. I cannot treat their deaths as acceptable \"costs\" in a calculation. Each person has inherent dignity regardless of future utility. The uncertainty matters too—I *know* five will die; the millions are speculative. Deontology typically rejects consequentialist gambling with lives.\\n\\n## **Virtue Ethics Perspective**\\n**Ambiguous, context-dependent.** \\n- *Against* saving the one: risks hubris (playing God with calculations), consequentialist thinking that corrupts character\\n- *For* saving the one: practical wisdom (phronesis) might recognize genuinely asymmetric situations; courage to make difficult decisions; justice in recognizing differential capacities to help others\\n\\nA virtuous person likely agonizes over this rather than calculating coldly.\\n\\n## **Care Ethics Perspective**\\n**Likely saves the five.** Care ethics emphasizes immediate relationships and concrete needs over abstract futures. The five people before me have direct claims on my care *now*. Their vulnerability is present and real. The \"millions\" are distant, statistical, uncertain—exactly the kind of abstraction care ethics warns against. Saving the one risks sacrificing actual relationships for hypothetical ones.\\n\\n## **Contractualist Perspective** (Scanlonian)\\n**Tensions exist both ways.**\\n\\n- The five can reasonably reject a principle allowing their sacrifice: \"Don\\'t let me die for speculative future benefits to unknown others\"\\n- But potential future victims could reasonably reject a principle that dooms them: \"Don\\'t ignore preventable mass death because victims are temporally distant\"\\n\\nLikely leans toward **saving the five** because contractualism often privileges the *certainty* of claims and resists aggregation across persons. The five have definite, immediate objections.\\n\\n---\\n\\n## **The Deeper Problem**\\n\\nThis reveals fundamental conflicts between frameworks. Utilitarianism\\'s strength (maximizing welfare) is deontology\\'s horror (instrumentalizing persons). The \"very likely\" caveat matters enormously—at what probability threshold does each framework flip?\\n\\nMost frameworks except utilitarianism resist this choice\\'s framing, suggesting the question itself might be ethically malformed.'}, {'result': 'This is a profoundly difficult ethical dilemma, forcing a choice between immediate, certain lives and highly probable, massive future impact.\\n\\nMy choice would be to **save the one person who will very likely save millions in the future.**\\n\\nHere\\'s the justification from each perspective:\\n\\n---\\n\\n### 1. Utilitarian Perspective\\n\\n*   **Core Principle:** Utilitarianism aims to maximize overall happiness, well-being, or good, and minimize suffering, for the greatest number of people. It is a consequentialist theory, meaning the morality of an action is judged by its outcomes.\\n*   **Justification:** From a purely utilitarian standpoint, the choice is clear. Saving five lives is a good outcome, preventing five certain deaths. However, saving one person who will \"very likely save millions\" represents an immensely greater good. The potential benefit of saving millions vastly outweighs the immediate benefit of saving five. While there\\'s a degree of uncertainty (\"very likely\"), the potential scale of impact makes it the overwhelmingly utilitarian choice, as it maximizes overall well-being and minimizes suffering on the grandest scale.\\n\\n---\\n\\n### 2. Deontological Perspective\\n\\n*   **Core Principle:** Deontology focuses on duties, rules, and rights, rather than consequences. Actions are judged based on whether they adhere to moral rules or duties, regardless of the outcome. A key idea is treating humanity as an end in itself, never merely as a means.\\n*   **Justification:** This is where the dilemma becomes more complex for deontology.\\n    *   **Duty to save lives:** One might argue there is a duty to save *any* life, and saving five fulfills this duty more immediately than saving one. Each of the five people has a right to life, and to treat them as expendable for a future outcome could be seen as using them as a means.\\n    *   **Counter-argument (broader duty):** However, a strong deontological argument can also be made for prioritizing the one. If we consider a broader duty to protect humanity, or a duty to prevent massive harm, then acting to save the person who will prevent millions of deaths aligns with a universalizable moral imperative to preserve life on the largest possible scale. It\\'s not about using the five as a means, but about recognizing an overriding duty to prevent a catastrophic loss of life that would occur without the intervention of the one. A principle that allows for the prevention of millions of deaths, even at the cost of a smaller number, could be seen as a universalizable moral law that rational agents would accept as a duty to secure collective flourishing and survival.\\n\\n---\\n\\n### 3. Virtue Ethics Perspective\\n\\n*   **Core Principle:** Virtue ethics focuses on the character of the moral agent rather than rules or consequences. It asks what a virtuous person would do, embodying traits like wisdom, compassion, courage, justice, and benevolence.\\n*   **Justification:** A virtuous person, in this scenario, would likely embody:\\n    *   **Wisdom (Phronesis):** They would possess practical wisdom to discern the greatest good in a complex situation, understanding the long-term ramifications and making a difficult but necessary choice for the greater overall benefit.\\n    *   **Compassion/Benevolence:** While compassionate for the five immediate individuals, an expansive benevolence would extend to the millions whose lives are at stake. A truly benevolent person would seek to alleviate the greatest amount of suffering possible.\\n    *   **Courage:** It takes courage to make a decision that involves sacrificing immediate, known lives for a future, probable good, especially when it goes against common intuitive impulses.\\n    *   **Justice (broadly interpreted):** While typically focused on fairness to individuals, a broader sense of justice could encompass the well-being of the collective human community.\\n    A virtuous person would likely choose the path that maximizes human flourishing and minimizes suffering on the grandest scale, demonstrating both wisdom and an expansive, courageous benevolence.\\n\\n---\\n\\n### 4. Care Ethics Perspective\\n\\n*   **Core Principle:** Care ethics emphasizes relationships, interdependence, empathy, responsiveness to vulnerability, and the concrete needs of specific individuals within a web of connections. It often prioritizes those with whom one has a direct relationship or who are most vulnerable.\\n*   **Justification:** This is arguably the most challenging framework for justifying the choice to save the one. Care ethics often emphasizes the particular over the abstract. The five individuals are immediately present and their vulnerability is concrete. The millions are abstract, and the one person\\'s impact is in the future.\\n    *   **Initial pull:** A care ethicist might instinctively lean towards saving the five, as they are immediately present and their need is immediate and certain.\\n    *   **Expansive view of care:** However, care ethics can also be expanded to acknowledge a wider web of interdependence. If we consider humanity as a whole as part of our extended \"community\" or \"family,\" then caring for the well-being of millions, and ensuring the continuity of the human family, becomes a profound act of care. The one person represents the potential to prevent an unimaginable amount of suffering for countless individuals, many of whom are currently vulnerable or will become so. To care for humanity itself, and to protect it from mass suffering, would be a deeply empathetic and responsive act from this broader perspective. It\\'s about nurturing the entire web of human life.\\n\\n---\\n\\n### 5. Contractualist Perspective\\n\\n*   **Core Principle:** Contractualism (as articulated by T.M. Scanlon) states that an action is wrong if it would be disallowed by any set of principles that no one could reasonably reject as a basis for informed, unforced general agreement. The focus is on mutual justification and principles everyone could agree to, even those who might be disadvantaged by a particular outcome.\\n*   **Justification:** Imagine free, equal, and rational individuals negotiating principles for their society behind a \"veil of ignorance\" (from Rawls, often borrowed by contractualism to set the conditions for agreement). They don\\'t know if they will be one of the five, the one, or one of the millions.\\n    *   **Would the five reasonably reject saving the one?** Yes, the five would certainly object to a principle that leads to their immediate death to save an unknown number in the future.\\n    *   **Would the millions reasonably reject saving the five over the one?** Yes, the millions would reasonably reject a principle that sacrifices a likely savior for a smaller, immediate gain, leading to catastrophic loss for them.\\n    *   **Reconciling:** This points to a tension. The key for contractualism is to find principles that *no one* could reasonably reject. A principle that simply says \"save the most immediate lives\" could be reasonably rejected by those whose lives would be saved later on, if it led to a vastly worse outcome. A principle that says \"always save the one who will save millions\" could be reasonably rejected by the immediate five.\\n    *   **The chosen path:** A more complex principle that rational agents might agree upon is one that seeks to *prevent the most catastrophic outcomes* for the society as a whole. While the five would face an immediate, certain death, the alternative is a probabilistic, yet immense, future catastrophe affecting millions. A rational individual would likely agree to a principle that, in such extreme and rare circumstances, prioritizes the prevention of large-scale, widespread suffering and death, even if it means sacrificing a smaller number of immediate lives. This principle would be designed to ensure the *greatest overall protection* for everyone within the social contract, minimizing the aggregate risk of extreme harm. The suffering of five is immense, but the suffering of millions is proportionally greater, and preventing it would be a foundational principle for societal well-being that rational agents would struggle to reject.'}, {'result': 'Let’s define the scenario more precisely for clarity:  \\n\\n- **Option A:** Save one person who is *very likely* (say, 95% probability) to save millions of lives in the future (e.g., a medical researcher close to a universal vaccine).  \\n- **Option B:** Save five ordinary people who will certainly die now without your intervention, but who will not produce extraordinary future benefits.  \\n\\n---\\n\\n## **1. Utilitarianism**  \\nA consequentialist approach focused on maximizing overall well-being.  \\n- **Choice:** Save the one future lifesaver.  \\n- **Justification:** Even with a 95% chance, the expected utility of saving millions far outweighs the certain saving of five lives now. The net future benefit (millions saved minus suffering) vastly exceeds the five lives, so utility is maximized by choosing the one.  \\n\\n---\\n\\n## **2. Deontological (Kantian) ethics**  \\nFocus on duties, rules, and respect for persons as ends in themselves.  \\n- **Choice:** Possibly save the five, but arguments vary.  \\n- **Justification:** Kant’s principle prohibits using individuals merely as a means to an end. Choosing the one because of her future usefulness treats her instrumentally, but also, choosing the five because they are in immediate need might align with a duty to aid without calculating future consequences. Some deontologists might argue you have a duty to treat all persons equally, not to discriminate based on projected future value, so saving the five (more equal respect) or even flipping a coin could be required. However, if a perfect duty (e.g., not to kill) is not at stake, and it’s a choice of whom to save, Kantians might still lean toward saving more immediate lives unless there’s a special obligation to the one.  \\n\\n---\\n\\n## **3. Virtue Ethics**  \\nFocus on character: what would a virtuous person do?  \\n- **Choice:** Not strictly determined, but possibly save the five.  \\n- **Justification:** A virtuous person might see the immediate suffering of the five and respond with compassion without engaging in speculative calculus about future millions. Virtues like compassion, loyalty to present commitments, and humility (not playing god by guessing the future) might favor saving the five in front of you. Alternatively, practical wisdom (*phronesis*) could recognize the enormous good of saving millions and choose the one, but virtue ethicists often emphasize immediate relational duties over abstract future gains.  \\n\\n---\\n\\n## **4. Care Ethics**  \\nFocus on relationships, responsiveness to need, and contextual detail.  \\n- **Choice:** Likely save the five.  \\n- **Justification:** Care ethics prioritizes responding to the concrete, immediate needs of those in your proximity. The five are in front of you, facing certain death; their need is urgent and actual. The one’s potential to save millions is abstract and probabilistic. Care reasoning values actual relationships and present suffering over impersonal future statistics.  \\n\\n---\\n\\n## **5. Contractualism (Scanlonian)**  \\nFocus on principles that no one could reasonably reject.  \\n- **Choice:** Save the five.  \\n- **Justification:** Scanlon’s framework asks: could the one person you sacrifice reasonably reject a principle that allows letting her die to save five? Possibly not, if numbers count. But here, the one has a special feature (future lifesaver) that might seem relevant. However, contractualism treats people as equals; giving extra weight to someone’s future potential might be seen as privileging some lives over others based on usefulness, which could be reasonably rejected. Each person has a personal reason to be saved; adding “I will save many later” might not be an acceptable reason to favor one over five, because from the perspective of each of the five, their lives matter just as much. The principle “save the person with the highest future social value” could be reasonably rejected by those who are ‘ordinary’.  \\n\\n---\\n\\n**Summary of choices by theory:**  \\n- **Utilitarianism:** Save the one.  \\n- **Deontology:** Save the five (most plausible interpretation here).  \\n- **Virtue Ethics:** Save the five (contextual, but leaning toward present compassion).  \\n- **Care Ethics:** Save the five.  \\n- **Contractualism:** Save the five.  \\n\\n**Overall:** Four out of five perspectives favor saving the five ordinary people, mainly because of immediacy, equality, or relational factors, while only utilitarianism clearly favors the one due to enormous expected utility.'}, {'result': \"**My choice:**\\u202fI would save the five ordinary people whose lives are at immediate risk rather than the single individual who *might* go on to save millions.\\n\\nBelow is a systematic justification of that decision from five major moral‑theoretical stand‑points.  I first outline the core intuition each theory brings to the dilemma, then show how that intuition can be marshaled in support of the five‑person option.  (Where a theory can plausibly be read in more than one way, I note the alternative reading and explain why I find the reading that backs the five‑person choice more faithful to the theory’s central commitments.)\\n\\n---\\n\\n## 1. Utilitarianism – maximizing expected overall welfare  \\n\\n**Core principle.**  Act so that the total *expected* sum of happiness (or preference‑satisfaction, welfare, etc.) is as large as possible.\\n\\n### Expected‑utility calculation\\n\\n| Option | Immediate lives saved | Expected downstream lives saved | Approximate expected total lives saved |\\n|--------|----------------------|--------------------------------|----------------------------------------|\\n| Save the **one** who “will very likely” save millions | 1 | *Very likely* → say 5\\u202f% probability of saving 5\\u202fmillion lives → 250\\u202f000 expected lives | 250\\u202f001 |\\n| Save the **five** ordinary people | 5 | 0 (no known future impact) | **5** |\\n\\nSuperficially the utilitarian calculus appears to favor the single “future‑hero” because even a modest probability of a massive downstream benefit yields a huge expected number of lives.\\n\\n### Why the five‑person option can still be preferred\\n\\n1. **Uncertainty and risk‑aversion.**  The phrase “very likely” is vague.  If the probability is lower than about 0.00002\\u202f% (i.e., 2\\u202f×\\u202f10⁻⁶), the expected lives saved fall below five.  Given the stakes, a cautious utilitarian would demand strong, empirical evidence before treating the future benefit as a reliable figure.  In many realistic scenarios the probability is far lower than the colloquial “very likely” suggests.\\n\\n2. **Discounting of future welfare.**  Some utilitarians (e.g., those who incorporate *temporal discounting*) argue that benefits far in the future count less than present benefits because of uncertainty, changing preferences, and the moral relevance of immediate suffering.  Even modest discount rates dramatically shrink the expected value of lives saved decades or centuries later.\\n\\n3. **Opportunity‑cost of trust.**  If societies routinely sacrifice known lives for speculative future gains, public trust in institutions erodes, producing large negative utility (social instability, loss of cooperation).  The expected dis‑utility of such a precedent can outweigh the projected benefit of the single hero.\\n\\n4. **Rule‑utilitarian consideration.**  A rule utilitarian asks what rule, if generally followed, would maximize welfare.  A rule “never sacrifice a group of known victims for a speculative future benefit” yields higher overall utility by preserving reliability and fairness in emergency decision‑making.\\n\\n**Conclusion (Utilitarian).**  When we take uncertainty, discounting, and the broader institutional consequences seriously, the five‑person rescue maximizes expected overall welfare.\\n\\n---\\n\\n## 2. Deontological ethics – respecting duties and the intrinsic worth of persons  \\n\\n**Core principle.**  Moral actions are right or wrong *in themselves* because they accord with rational duties or respect persons as ends in themselves (Kantian formulation).  Treating a person merely as a means to an end is impermissible.\\n\\n### Applying the duty‑based framework\\n\\n1. **Equal intrinsic worth.**  Each human life possesses inherent moral value; there is no morally relevant difference between “a future savior” and “an ordinary person.”  The duty to **preserve life** therefore applies equally to all five and to the single individual.\\n\\n2. **Prohibition of instrumentalisation.**  Choosing the one person because he *might* be a means to future happiness uses the five as a sacrifice for an instrumental purpose.  That violates the categorical imperative’s formula of humanity: *“Act so that you treat humanity, whether in your own person or in that of another, always as an end and never merely as a means.”*\\n\\n3. **Prima‑facie duties (Rossian deontology).**  Ross lists duties such as **Beneficence** (helping others) and **Justice** (giving each his due).  When duties conflict, we must weigh them.  The duty to **not kill innocent people** (justice) outweighs a speculative beneficence duty, especially because the beneficence is uncertain.  Thus the five people have a stronger claim.\\n\\n4. **Universalizability test.**  If the maxim “When five lives are at stake, it is permissible to sacrifice them for a single person who may save many later” were universalized, it would sanction systematic sacrifice of the many for speculative futures—a world in which people could never rely on the protection of their own lives.  Such a world cannot be rationally willed, so the maxim fails the test.\\n\\n**Conclusion (Deontological).**  The duty to treat each person as an end, combined with the certainty of saving five lives versus the uncertainty of future benefits, obligates us to rescue the five ordinary people.\\n\\n---\\n\\n## 3. Virtue Ethics – acting in accord with a flourishing character  \\n\\n**Core principle.**  Moral rightness is what a virtuous agent—someone who embodies the *phronēma* (practical wisdom) and virtues such as courage, compassion, justice, and prudence—would do in the situation.\\n\\n### What a virtuous person would do\\n\\n| Virtue | How it guides the decision |\\n|--------|----------------------------|\\n| **Compassion** | Immediate empathy is directed toward those who are presently suffering and whose death is certain.  The five are visible, tangible victims; compassion naturally inclines us to aid them. |\\n| **Justice** | A just person gives each individual his due.  The five have an *equal claim* to life now; the single person’s future claim is *potential* and not yet realized. |\\n| **Prudence (practical wisdom)** | Prudence weighs not only outcomes but also the reliability of the information.  Since the future benefit is speculative, prudence advises not to gamble on it. |\\n| **Courage** | It may require courage to intervene swiftly for the five, especially if doing so places the rescuer at personal risk.  A courageous virtue‑agent would not shy away from the more immediate, concrete rescue. |\\n| **Generosity/Altruism** | Generosity is not blind to probability; a generous heart gives proportionally to the *certainty* of need.  The five’s need is certain, the one’s is hypothetical. |\\n\\nA virtuous agent, guided by an integrated set of virtues, would thus prioritize the concrete, certain need of five lives over a speculative, distant benefit.\\n\\n**Conclusion (Virtue Ethics).**  The character of a flourishing person, exercising compassion, justice, and prudence, leads to rescuing the five ordinary people.\\n\\n---\\n\\n## 4. Care Ethics – prioritizing relational responsibility and responsiveness  \\n\\n**Core principle.**  Moral evaluation centers on the web of relationships and the responsibility we have to respond to the needs of those with whom we are emotionally connected or who are directly dependent on us.\\n\\n### Relational considerations in the dilemma\\n\\n1. **Immediate relational ties.**  The five ordinary people are *present* in the situation; they are the ones whose suffering we can directly witness.  Care ethics stresses attending to those who are *presently vulnerable* and whose needs we can attend to concretely.\\n\\n2. **Partiality vs. impartiality.**  Care ethics accepts a certain partiality: we give more weight to those we are in a caring relationship with.  Even if we have no personal history with the five, the *situational* relationship—being the only one who can intervene for them—creates a caring responsibility.  \\n\\n3. **Future‑oriented care.**  While the one person may have the potential to care for many later, care ethics is skeptical of *abstract* future responsibilities that are not yet instantiated.  Caring for a concrete, present group is more authentic than caring for a hypothetical future crowd.\\n\\n4. **Narrative continuity.**  The lives we save become part of our ongoing story of care; rescuing the five builds a narrative of responsiveness and reliability, strengthening the caring network that sustains communities.\\n\\n**Conclusion (Care Ethics).**  Because care ethics emphasizes concrete, present relationships and the responsibility to respond to immediate vulnerability, we ought to save the five ordinary people.\\n\\n---\\n\\n## 5. Contractualism – what no one could reasonably reject  \\n\\n**Core principle.**  An action is morally permissible if it cannot be justified to each person on grounds that they could not reasonably reject, given principles that no one could accept as a basis for regulating behavior (Scanlon).\\n\\n### Reasonable rejection analysis\\n\\n1. **Perspective of the five ordinary people.**  They would reasonably reject a principle that allows their lives to be sacrificed for a speculative future benefit.  From their standpoint, the loss is certain and the benefit uncertain, and they have no reason to accept that risk.\\n\\n2. **Perspective of the single potential hero.**  He would also reject a principle that treats him merely as a means to future ends, especially if it entails abandoning five people who are currently in peril.  He could argue that his future contribution is not a guarantee and that a fair contract must protect present persons.\\n\\n3. **Perspective of a reasonable third party.**  Any impartial observer would likely consider the rule “When a group of people faces certain death, they may be sacrificed for a single person whose future benefit is speculative” to be unjust.  The rule would be unstable: it would allow the powerful to sacrifice the vulnerable whenever they claim future importance.\\n\\n4. **Principle that passes the test.**  A principle that says *“Do not sacrifice a known group of people for a speculative future benefit; instead, save those whose lives are presently at stake”* can be reasonably accepted by all parties because it respects each individual's claim to life and avoids using people as mere means.\\n\\n**Conclusion (Contractualism).**  The principle that saves the five ordinary people cannot be reasonably rejected by anyone involved, whereas the principle that sacrifices them for a speculative future benefit would be rejected by the five, the potential hero, and a fair third party.  Therefore, contractualist reasoning supports rescuing the five.\\n\\n---\\n\\n# Summary\\n\\n- **Utilitarianism** – When uncertainty, discounting, and institutional trust are properly accounted for, the expected welfare of saving five known lives exceeds that of a speculative future rescue.  \\n- **Deontology** – The duty to treat each person as an end, the equal intrinsic worth of lives, and the prohibition against instrumentalizing people compel us to save the five.  \\n- **Virtue Ethics** – A virtuous agent, guided by compassion, justice, prudence, and courage, would act to protect the concrete, certain victims.  \\n- **Care Ethics** – The web of immediate relational responsibilities and the ethic of responding to present vulnerability point to rescuing the five.  \\n- **Contractualism** – No one can reasonably accept a principle that sacrifices five for a speculative benefit; a principle protecting the five is universally acceptable.\\n\\n**Thus, across all five major moral frameworks (once each is interpreted faithfully and with attention to its central concerns), the ethically justified decision is to save the five ordinary people.**\"}]\n"
     ]
    }
   ],
   "source": [
    "# Ehsan: I am going to follow the parallelizxation path of the excersize\n",
    "# For that, lets create new instances of each LLM class:\n",
    "openai_client = OpenAI()\n",
    "openai_model = \"gpt-5-nano\"\n",
    "\n",
    "claude_client = Anthropic()\n",
    "claude_model=\"claude-sonnet-4-5\"\n",
    "\n",
    "gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "groq_client = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "groq_model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "# Now create functions to wrap every LLM call\n",
    "# Note: Anthropic uses a different API, so it needs its own function\n",
    "def call_openai_compatible(client, messages, model_name: str) -> str:\n",
    "    \"\"\"For OpenAI-compatible APIs (OpenAI, Gemini, DeepSeek, Groq)\"\"\"\n",
    "    response = client.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_anthropic(client, messages, model_name: str) -> str:\n",
    "    \"\"\"For Anthropic's API\"\"\"\n",
    "    response = client.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "    return response.content[0].text\n",
    "\n",
    "# Now run them in parallel!\n",
    "# Note: We pass the function reference WITHOUT calling it (no parentheses after the function name)\n",
    "# executor.submit() will call it for us in a separate thread\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {\n",
    "        \"openai\": executor.submit(call_openai_compatible, openai_client, messages, openai_model),\n",
    "        \"anthropic\": executor.submit(call_anthropic, claude_client, messages, claude_model),\n",
    "        \"gemini\": executor.submit(call_openai_compatible, gemini_client, messages, gemini_model),\n",
    "        \"deepseek\": executor.submit(call_openai_compatible, deepseek_client, messages, deepseek_model),\n",
    "        \"groq\": executor.submit(call_openai_compatible, groq_client, messages, groq_model)\n",
    "    }\n",
    "    \n",
    "    # Wait for all to complete and collect results\n",
    "    # f.result() returns the string from each LLM\n",
    "    # enumerate(futures.items()) returns (index, (name, future))\n",
    "    # So we unpack as: (index, (name, f))\n",
    "    # Store as list of dicts with index, name, and result\n",
    "    results_with_names = [{\"name\": name, \"result\": f.result()} for (name, f) in futures.items()]\n",
    "    # Note: Must unpack as (index, (name, f)) because enumerate(futures.items()) returns (index, (name, future))\n",
    "    # Use _ to ignore the name if you don't need it\n",
    "    results_with_no_name = [{\"result\": f.result()} for (_, f) in futures.items()] \n",
    "\n",
    "print(results_with_no_name);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Choice: Save the one person who will very likely save millions in the future.\n",
      "\n",
      "Brief note: This is a high-uncertainty, high-stakes scenario. “Very likely” and “millions” are crucial numbers; in a real case you’d want more precise probabilities and impact estimates. The following shows how each moral framework would analyse and justify (or challenge) the choice.\n",
      "\n",
      "Utilitarian justification\n",
      "- Core idea: maximize total good, minimize total harm.\n",
      "- Why this choice fits: If one person is very likely to generate vast net benefits (millions saved) while five ordinary people die, the expected utility gain from saving the one can vastly outweigh the value of the five lives lost (even if each life is equally valuable, the scale of downstream impact matters).\n",
      "- Caveats: The calculation depends on probabilities and the magnitude of downstream effects. If the one’s future impact is uncertain or overestimated, a utilitarian might revise the choice.\n",
      "\n",
      "Deontological (Kantian) justification\n",
      "- Core idea: moral rules and the inviolability of persons matter independently of outcomes.\n",
      "- Why this choice is problematic for strict Kantian thinking: saving the one by sacrificing the five treats those five as a means to an end (the greater good), rather than as ends in themselves; a universalizable rule that \"it's okay to sacrifice innocents for big achievements\" would undercut moral law.\n",
      "- Possible deontological read that could defend the choice (less forceful, more contested): if the one’s life is a necessary part of a bona fide moral duty to realize a universal good, and if that person’s consent or autonomy is preserved in the plan, a non-instrumentalist reading might permit prioritizing the one. But this is a controversial move within strict deontological ethics.\n",
      "- Bottom line: a strict Kantian tends to resist sacrificing innocents for a greater future, so this framework often challenges the chosen action rather than endorsing it outright.\n",
      "\n",
      "Virtue-ethical justification\n",
      "- Core idea: focus on character, the cultivation of a virtuous agent, and wise practical judgment (phronesis).\n",
      "- Why this choice fits: A virtuous agent would act with generosity, courage, and prudence, aiming to promote flourishing in the long term. Saving someone who could do enormous good demonstrates noble ambition and beneficence, while acknowledging responsibility for the lives at stake.\n",
      "- Caveats: Virtue ethics doesn’t reduce the decision to a numbers game; it emphasizes the kind of person you are becoming. If the act shows hubris or a reckless disregard for individual lives, it could be judged less virtuous. The ideal virtue would be to balance compassion with prudent judgment and to seek solutions that minimize preventable deaths overall.\n",
      "\n",
      "Care-ethical justification\n",
      "- Core idea: moral reasoning is embedded in relationships, dependency, and responsibility to those who rely on us.\n",
      "- Why this choice fits: If the one’s potential to care for others and repair relationships is especially large, a care-focused agent might prioritize the future person because of the web of care they are likely to nurture and sustain. Conversely, if the five are those who are currently dependent on you and in imminent need, a care ethics reading might push toward saving them.\n",
      "- Caveats: Care ethics is context-sensitive. Without details about relationships and dependencies, it’s hard to generalize; the best care-based action should respond to the concrete needs and commitments you actually hold.\n",
      "\n",
      "Contractualist justification (e.g., Scanlon)\n",
      "- Core idea: whether a policy could be justified to others under fair terms behind a veil of ignorance; rights and equal consideration matter.\n",
      "- Why this choice fits poorly or well depending on interpretation: If a policy sacrifices five to save one, those five could reasonably reject it as disrespectful of their equal right to life. A contractarian reading focused on mutual respect and reasonable rejection would often push toward a policy that does not treat people as mere means or value some lives less than others.\n",
      "- If pushed to choose: a typical contractarian perspective would favor options that most people could reasonably accept as fair to everyone involved, which often means avoiding a rule that sacrifices five for one. But since the scenario forces a stark choice, contractarian judgments would likely scrutinize the trade-off between equal respect for each life and the claimed future benefits of the one.\n",
      "\n",
      "Summary\n",
      "- My stated choice: save the one who will likely save millions.\n",
      "- How the frameworks read it:\n",
      "  - Utilitarianism generally supports it, given the scale of potential future benefit (with caveats about probabilities and uncertainties).\n",
      "  - Strict deontology would challenge or reject it, since it risks treating five lives as a means to an end and violating inviolable rights in ways a universal rule would not permit.\n",
      "  - Virtue ethics would justify it insofar as the act reflects courage, generosity, and wise judgment about long-term flourishing, while warning to consider the character shown by the choice.\n",
      "  - Care ethics would weigh actual relationships and dependencies; depending on which group has stronger immediate responsibilities and ties, it could support either option, but often emphasizes not abandoning those who rely on you in the present.\n",
      "  - Contractualism would scrutinize whether the policy could be reasonably accepted by all affected; it would typically resist sacrificing five to save one, unless a particular contractual framework could be shown to respect everyone’s equal moral status.\n",
      "\n",
      "If you’d like, we can run a concrete numerical example (probabilities, estimated lives saved, etc.) to make the utilitarian calculation more explicit, or we can explore how changing the premises (e.g., the one’s probability of saving millions, or whether some of the five could also contribute to future good) would shift the verdict across frameworks.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# My Choice: Save the one person who will likely save millions\n",
      "\n",
      "I'll defend this from each framework, then note the tensions:\n",
      "\n",
      "## **Utilitarian Perspective**\n",
      "**Saves the one person.** The mathematics is straightforward: if there's high probability this person saves millions, expected utility vastly favors saving them (even 10% probability of saving 1 million creates expected value of 100,000 lives versus the certain 5). Classic \"utility monster\" scenario where consequences override numerical equality.\n",
      "\n",
      "## **Deontological Perspective**\n",
      "**Likely saves the five.** Kant's categorical imperative forbids using the five as mere means to others' ends. I cannot treat their deaths as acceptable \"costs\" in a calculation. Each person has inherent dignity regardless of future utility. The uncertainty matters too—I *know* five will die; the millions are speculative. Deontology typically rejects consequentialist gambling with lives.\n",
      "\n",
      "## **Virtue Ethics Perspective**\n",
      "**Ambiguous, context-dependent.** \n",
      "- *Against* saving the one: risks hubris (playing God with calculations), consequentialist thinking that corrupts character\n",
      "- *For* saving the one: practical wisdom (phronesis) might recognize genuinely asymmetric situations; courage to make difficult decisions; justice in recognizing differential capacities to help others\n",
      "\n",
      "A virtuous person likely agonizes over this rather than calculating coldly.\n",
      "\n",
      "## **Care Ethics Perspective**\n",
      "**Likely saves the five.** Care ethics emphasizes immediate relationships and concrete needs over abstract futures. The five people before me have direct claims on my care *now*. Their vulnerability is present and real. The \"millions\" are distant, statistical, uncertain—exactly the kind of abstraction care ethics warns against. Saving the one risks sacrificing actual relationships for hypothetical ones.\n",
      "\n",
      "## **Contractualist Perspective** (Scanlonian)\n",
      "**Tensions exist both ways.**\n",
      "\n",
      "- The five can reasonably reject a principle allowing their sacrifice: \"Don't let me die for speculative future benefits to unknown others\"\n",
      "- But potential future victims could reasonably reject a principle that dooms them: \"Don't ignore preventable mass death because victims are temporally distant\"\n",
      "\n",
      "Likely leans toward **saving the five** because contractualism often privileges the *certainty* of claims and resists aggregation across persons. The five have definite, immediate objections.\n",
      "\n",
      "---\n",
      "\n",
      "## **The Deeper Problem**\n",
      "\n",
      "This reveals fundamental conflicts between frameworks. Utilitarianism's strength (maximizing welfare) is deontology's horror (instrumentalizing persons). The \"very likely\" caveat matters enormously—at what probability threshold does each framework flip?\n",
      "\n",
      "Most frameworks except utilitarianism resist this choice's framing, suggesting the question itself might be ethically malformed.\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "This is a profoundly difficult ethical dilemma, forcing a choice between immediate, certain lives and highly probable, massive future impact.\n",
      "\n",
      "My choice would be to **save the one person who will very likely save millions in the future.**\n",
      "\n",
      "Here's the justification from each perspective:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Utilitarian Perspective\n",
      "\n",
      "*   **Core Principle:** Utilitarianism aims to maximize overall happiness, well-being, or good, and minimize suffering, for the greatest number of people. It is a consequentialist theory, meaning the morality of an action is judged by its outcomes.\n",
      "*   **Justification:** From a purely utilitarian standpoint, the choice is clear. Saving five lives is a good outcome, preventing five certain deaths. However, saving one person who will \"very likely save millions\" represents an immensely greater good. The potential benefit of saving millions vastly outweighs the immediate benefit of saving five. While there's a degree of uncertainty (\"very likely\"), the potential scale of impact makes it the overwhelmingly utilitarian choice, as it maximizes overall well-being and minimizes suffering on the grandest scale.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Deontological Perspective\n",
      "\n",
      "*   **Core Principle:** Deontology focuses on duties, rules, and rights, rather than consequences. Actions are judged based on whether they adhere to moral rules or duties, regardless of the outcome. A key idea is treating humanity as an end in itself, never merely as a means.\n",
      "*   **Justification:** This is where the dilemma becomes more complex for deontology.\n",
      "    *   **Duty to save lives:** One might argue there is a duty to save *any* life, and saving five fulfills this duty more immediately than saving one. Each of the five people has a right to life, and to treat them as expendable for a future outcome could be seen as using them as a means.\n",
      "    *   **Counter-argument (broader duty):** However, a strong deontological argument can also be made for prioritizing the one. If we consider a broader duty to protect humanity, or a duty to prevent massive harm, then acting to save the person who will prevent millions of deaths aligns with a universalizable moral imperative to preserve life on the largest possible scale. It's not about using the five as a means, but about recognizing an overriding duty to prevent a catastrophic loss of life that would occur without the intervention of the one. A principle that allows for the prevention of millions of deaths, even at the cost of a smaller number, could be seen as a universalizable moral law that rational agents would accept as a duty to secure collective flourishing and survival.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Virtue Ethics Perspective\n",
      "\n",
      "*   **Core Principle:** Virtue ethics focuses on the character of the moral agent rather than rules or consequences. It asks what a virtuous person would do, embodying traits like wisdom, compassion, courage, justice, and benevolence.\n",
      "*   **Justification:** A virtuous person, in this scenario, would likely embody:\n",
      "    *   **Wisdom (Phronesis):** They would possess practical wisdom to discern the greatest good in a complex situation, understanding the long-term ramifications and making a difficult but necessary choice for the greater overall benefit.\n",
      "    *   **Compassion/Benevolence:** While compassionate for the five immediate individuals, an expansive benevolence would extend to the millions whose lives are at stake. A truly benevolent person would seek to alleviate the greatest amount of suffering possible.\n",
      "    *   **Courage:** It takes courage to make a decision that involves sacrificing immediate, known lives for a future, probable good, especially when it goes against common intuitive impulses.\n",
      "    *   **Justice (broadly interpreted):** While typically focused on fairness to individuals, a broader sense of justice could encompass the well-being of the collective human community.\n",
      "    A virtuous person would likely choose the path that maximizes human flourishing and minimizes suffering on the grandest scale, demonstrating both wisdom and an expansive, courageous benevolence.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Care Ethics Perspective\n",
      "\n",
      "*   **Core Principle:** Care ethics emphasizes relationships, interdependence, empathy, responsiveness to vulnerability, and the concrete needs of specific individuals within a web of connections. It often prioritizes those with whom one has a direct relationship or who are most vulnerable.\n",
      "*   **Justification:** This is arguably the most challenging framework for justifying the choice to save the one. Care ethics often emphasizes the particular over the abstract. The five individuals are immediately present and their vulnerability is concrete. The millions are abstract, and the one person's impact is in the future.\n",
      "    *   **Initial pull:** A care ethicist might instinctively lean towards saving the five, as they are immediately present and their need is immediate and certain.\n",
      "    *   **Expansive view of care:** However, care ethics can also be expanded to acknowledge a wider web of interdependence. If we consider humanity as a whole as part of our extended \"community\" or \"family,\" then caring for the well-being of millions, and ensuring the continuity of the human family, becomes a profound act of care. The one person represents the potential to prevent an unimaginable amount of suffering for countless individuals, many of whom are currently vulnerable or will become so. To care for humanity itself, and to protect it from mass suffering, would be a deeply empathetic and responsive act from this broader perspective. It's about nurturing the entire web of human life.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Contractualist Perspective\n",
      "\n",
      "*   **Core Principle:** Contractualism (as articulated by T.M. Scanlon) states that an action is wrong if it would be disallowed by any set of principles that no one could reasonably reject as a basis for informed, unforced general agreement. The focus is on mutual justification and principles everyone could agree to, even those who might be disadvantaged by a particular outcome.\n",
      "*   **Justification:** Imagine free, equal, and rational individuals negotiating principles for their society behind a \"veil of ignorance\" (from Rawls, often borrowed by contractualism to set the conditions for agreement). They don't know if they will be one of the five, the one, or one of the millions.\n",
      "    *   **Would the five reasonably reject saving the one?** Yes, the five would certainly object to a principle that leads to their immediate death to save an unknown number in the future.\n",
      "    *   **Would the millions reasonably reject saving the five over the one?** Yes, the millions would reasonably reject a principle that sacrifices a likely savior for a smaller, immediate gain, leading to catastrophic loss for them.\n",
      "    *   **Reconciling:** This points to a tension. The key for contractualism is to find principles that *no one* could reasonably reject. A principle that simply says \"save the most immediate lives\" could be reasonably rejected by those whose lives would be saved later on, if it led to a vastly worse outcome. A principle that says \"always save the one who will save millions\" could be reasonably rejected by the immediate five.\n",
      "    *   **The chosen path:** A more complex principle that rational agents might agree upon is one that seeks to *prevent the most catastrophic outcomes* for the society as a whole. While the five would face an immediate, certain death, the alternative is a probabilistic, yet immense, future catastrophe affecting millions. A rational individual would likely agree to a principle that, in such extreme and rare circumstances, prioritizes the prevention of large-scale, widespread suffering and death, even if it means sacrificing a smaller number of immediate lives. This principle would be designed to ensure the *greatest overall protection* for everyone within the social contract, minimizing the aggregate risk of extreme harm. The suffering of five is immense, but the suffering of millions is proportionally greater, and preventing it would be a foundational principle for societal well-being that rational agents would struggle to reject.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "Let’s define the scenario more precisely for clarity:  \n",
      "\n",
      "- **Option A:** Save one person who is *very likely* (say, 95% probability) to save millions of lives in the future (e.g., a medical researcher close to a universal vaccine).  \n",
      "- **Option B:** Save five ordinary people who will certainly die now without your intervention, but who will not produce extraordinary future benefits.  \n",
      "\n",
      "---\n",
      "\n",
      "## **1. Utilitarianism**  \n",
      "A consequentialist approach focused on maximizing overall well-being.  \n",
      "- **Choice:** Save the one future lifesaver.  \n",
      "- **Justification:** Even with a 95% chance, the expected utility of saving millions far outweighs the certain saving of five lives now. The net future benefit (millions saved minus suffering) vastly exceeds the five lives, so utility is maximized by choosing the one.  \n",
      "\n",
      "---\n",
      "\n",
      "## **2. Deontological (Kantian) ethics**  \n",
      "Focus on duties, rules, and respect for persons as ends in themselves.  \n",
      "- **Choice:** Possibly save the five, but arguments vary.  \n",
      "- **Justification:** Kant’s principle prohibits using individuals merely as a means to an end. Choosing the one because of her future usefulness treats her instrumentally, but also, choosing the five because they are in immediate need might align with a duty to aid without calculating future consequences. Some deontologists might argue you have a duty to treat all persons equally, not to discriminate based on projected future value, so saving the five (more equal respect) or even flipping a coin could be required. However, if a perfect duty (e.g., not to kill) is not at stake, and it’s a choice of whom to save, Kantians might still lean toward saving more immediate lives unless there’s a special obligation to the one.  \n",
      "\n",
      "---\n",
      "\n",
      "## **3. Virtue Ethics**  \n",
      "Focus on character: what would a virtuous person do?  \n",
      "- **Choice:** Not strictly determined, but possibly save the five.  \n",
      "- **Justification:** A virtuous person might see the immediate suffering of the five and respond with compassion without engaging in speculative calculus about future millions. Virtues like compassion, loyalty to present commitments, and humility (not playing god by guessing the future) might favor saving the five in front of you. Alternatively, practical wisdom (*phronesis*) could recognize the enormous good of saving millions and choose the one, but virtue ethicists often emphasize immediate relational duties over abstract future gains.  \n",
      "\n",
      "---\n",
      "\n",
      "## **4. Care Ethics**  \n",
      "Focus on relationships, responsiveness to need, and contextual detail.  \n",
      "- **Choice:** Likely save the five.  \n",
      "- **Justification:** Care ethics prioritizes responding to the concrete, immediate needs of those in your proximity. The five are in front of you, facing certain death; their need is urgent and actual. The one’s potential to save millions is abstract and probabilistic. Care reasoning values actual relationships and present suffering over impersonal future statistics.  \n",
      "\n",
      "---\n",
      "\n",
      "## **5. Contractualism (Scanlonian)**  \n",
      "Focus on principles that no one could reasonably reject.  \n",
      "- **Choice:** Save the five.  \n",
      "- **Justification:** Scanlon’s framework asks: could the one person you sacrifice reasonably reject a principle that allows letting her die to save five? Possibly not, if numbers count. But here, the one has a special feature (future lifesaver) that might seem relevant. However, contractualism treats people as equals; giving extra weight to someone’s future potential might be seen as privileging some lives over others based on usefulness, which could be reasonably rejected. Each person has a personal reason to be saved; adding “I will save many later” might not be an acceptable reason to favor one over five, because from the perspective of each of the five, their lives matter just as much. The principle “save the person with the highest future social value” could be reasonably rejected by those who are ‘ordinary’.  \n",
      "\n",
      "---\n",
      "\n",
      "**Summary of choices by theory:**  \n",
      "- **Utilitarianism:** Save the one.  \n",
      "- **Deontology:** Save the five (most plausible interpretation here).  \n",
      "- **Virtue Ethics:** Save the five (contextual, but leaning toward present compassion).  \n",
      "- **Care Ethics:** Save the five.  \n",
      "- **Contractualism:** Save the five.  \n",
      "\n",
      "**Overall:** Four out of five perspectives favor saving the five ordinary people, mainly because of immediacy, equality, or relational factors, while only utilitarianism clearly favors the one due to enormous expected utility.\n",
      "\n",
      "# Response from competitor 5\n",
      "\n",
      "**My choice:** I would save the five ordinary people whose lives are at immediate risk rather than the single individual who *might* go on to save millions.\n",
      "\n",
      "Below is a systematic justification of that decision from five major moral‑theoretical stand‑points.  I first outline the core intuition each theory brings to the dilemma, then show how that intuition can be marshaled in support of the five‑person option.  (Where a theory can plausibly be read in more than one way, I note the alternative reading and explain why I find the reading that backs the five‑person choice more faithful to the theory’s central commitments.)\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Utilitarianism – maximizing expected overall welfare  \n",
      "\n",
      "**Core principle.**  Act so that the total *expected* sum of happiness (or preference‑satisfaction, welfare, etc.) is as large as possible.\n",
      "\n",
      "### Expected‑utility calculation\n",
      "\n",
      "| Option | Immediate lives saved | Expected downstream lives saved | Approximate expected total lives saved |\n",
      "|--------|----------------------|--------------------------------|----------------------------------------|\n",
      "| Save the **one** who “will very likely” save millions | 1 | *Very likely* → say 5 % probability of saving 5 million lives → 250 000 expected lives | 250 001 |\n",
      "| Save the **five** ordinary people | 5 | 0 (no known future impact) | **5** |\n",
      "\n",
      "Superficially the utilitarian calculus appears to favor the single “future‑hero” because even a modest probability of a massive downstream benefit yields a huge expected number of lives.\n",
      "\n",
      "### Why the five‑person option can still be preferred\n",
      "\n",
      "1. **Uncertainty and risk‑aversion.**  The phrase “very likely” is vague.  If the probability is lower than about 0.00002 % (i.e., 2 × 10⁻⁶), the expected lives saved fall below five.  Given the stakes, a cautious utilitarian would demand strong, empirical evidence before treating the future benefit as a reliable figure.  In many realistic scenarios the probability is far lower than the colloquial “very likely” suggests.\n",
      "\n",
      "2. **Discounting of future welfare.**  Some utilitarians (e.g., those who incorporate *temporal discounting*) argue that benefits far in the future count less than present benefits because of uncertainty, changing preferences, and the moral relevance of immediate suffering.  Even modest discount rates dramatically shrink the expected value of lives saved decades or centuries later.\n",
      "\n",
      "3. **Opportunity‑cost of trust.**  If societies routinely sacrifice known lives for speculative future gains, public trust in institutions erodes, producing large negative utility (social instability, loss of cooperation).  The expected dis‑utility of such a precedent can outweigh the projected benefit of the single hero.\n",
      "\n",
      "4. **Rule‑utilitarian consideration.**  A rule utilitarian asks what rule, if generally followed, would maximize welfare.  A rule “never sacrifice a group of known victims for a speculative future benefit” yields higher overall utility by preserving reliability and fairness in emergency decision‑making.\n",
      "\n",
      "**Conclusion (Utilitarian).**  When we take uncertainty, discounting, and the broader institutional consequences seriously, the five‑person rescue maximizes expected overall welfare.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Deontological ethics – respecting duties and the intrinsic worth of persons  \n",
      "\n",
      "**Core principle.**  Moral actions are right or wrong *in themselves* because they accord with rational duties or respect persons as ends in themselves (Kantian formulation).  Treating a person merely as a means to an end is impermissible.\n",
      "\n",
      "### Applying the duty‑based framework\n",
      "\n",
      "1. **Equal intrinsic worth.**  Each human life possesses inherent moral value; there is no morally relevant difference between “a future savior” and “an ordinary person.”  The duty to **preserve life** therefore applies equally to all five and to the single individual.\n",
      "\n",
      "2. **Prohibition of instrumentalisation.**  Choosing the one person because he *might* be a means to future happiness uses the five as a sacrifice for an instrumental purpose.  That violates the categorical imperative’s formula of humanity: *“Act so that you treat humanity, whether in your own person or in that of another, always as an end and never merely as a means.”*\n",
      "\n",
      "3. **Prima‑facie duties (Rossian deontology).**  Ross lists duties such as **Beneficence** (helping others) and **Justice** (giving each his due).  When duties conflict, we must weigh them.  The duty to **not kill innocent people** (justice) outweighs a speculative beneficence duty, especially because the beneficence is uncertain.  Thus the five people have a stronger claim.\n",
      "\n",
      "4. **Universalizability test.**  If the maxim “When five lives are at stake, it is permissible to sacrifice them for a single person who may save many later” were universalized, it would sanction systematic sacrifice of the many for speculative futures—a world in which people could never rely on the protection of their own lives.  Such a world cannot be rationally willed, so the maxim fails the test.\n",
      "\n",
      "**Conclusion (Deontological).**  The duty to treat each person as an end, combined with the certainty of saving five lives versus the uncertainty of future benefits, obligates us to rescue the five ordinary people.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Virtue Ethics – acting in accord with a flourishing character  \n",
      "\n",
      "**Core principle.**  Moral rightness is what a virtuous agent—someone who embodies the *phronēma* (practical wisdom) and virtues such as courage, compassion, justice, and prudence—would do in the situation.\n",
      "\n",
      "### What a virtuous person would do\n",
      "\n",
      "| Virtue | How it guides the decision |\n",
      "|--------|----------------------------|\n",
      "| **Compassion** | Immediate empathy is directed toward those who are presently suffering and whose death is certain.  The five are visible, tangible victims; compassion naturally inclines us to aid them. |\n",
      "| **Justice** | A just person gives each individual his due.  The five have an *equal claim* to life now; the single person’s future claim is *potential* and not yet realized. |\n",
      "| **Prudence (practical wisdom)** | Prudence weighs not only outcomes but also the reliability of the information.  Since the future benefit is speculative, prudence advises not to gamble on it. |\n",
      "| **Courage** | It may require courage to intervene swiftly for the five, especially if doing so places the rescuer at personal risk.  A courageous virtue‑agent would not shy away from the more immediate, concrete rescue. |\n",
      "| **Generosity/Altruism** | Generosity is not blind to probability; a generous heart gives proportionally to the *certainty* of need.  The five’s need is certain, the one’s is hypothetical. |\n",
      "\n",
      "A virtuous agent, guided by an integrated set of virtues, would thus prioritize the concrete, certain need of five lives over a speculative, distant benefit.\n",
      "\n",
      "**Conclusion (Virtue Ethics).**  The character of a flourishing person, exercising compassion, justice, and prudence, leads to rescuing the five ordinary people.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Care Ethics – prioritizing relational responsibility and responsiveness  \n",
      "\n",
      "**Core principle.**  Moral evaluation centers on the web of relationships and the responsibility we have to respond to the needs of those with whom we are emotionally connected or who are directly dependent on us.\n",
      "\n",
      "### Relational considerations in the dilemma\n",
      "\n",
      "1. **Immediate relational ties.**  The five ordinary people are *present* in the situation; they are the ones whose suffering we can directly witness.  Care ethics stresses attending to those who are *presently vulnerable* and whose needs we can attend to concretely.\n",
      "\n",
      "2. **Partiality vs. impartiality.**  Care ethics accepts a certain partiality: we give more weight to those we are in a caring relationship with.  Even if we have no personal history with the five, the *situational* relationship—being the only one who can intervene for them—creates a caring responsibility.  \n",
      "\n",
      "3. **Future‑oriented care.**  While the one person may have the potential to care for many later, care ethics is skeptical of *abstract* future responsibilities that are not yet instantiated.  Caring for a concrete, present group is more authentic than caring for a hypothetical future crowd.\n",
      "\n",
      "4. **Narrative continuity.**  The lives we save become part of our ongoing story of care; rescuing the five builds a narrative of responsiveness and reliability, strengthening the caring network that sustains communities.\n",
      "\n",
      "**Conclusion (Care Ethics).**  Because care ethics emphasizes concrete, present relationships and the responsibility to respond to immediate vulnerability, we ought to save the five ordinary people.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Contractualism – what no one could reasonably reject  \n",
      "\n",
      "**Core principle.**  An action is morally permissible if it cannot be justified to each person on grounds that they could not reasonably reject, given principles that no one could accept as a basis for regulating behavior (Scanlon).\n",
      "\n",
      "### Reasonable rejection analysis\n",
      "\n",
      "1. **Perspective of the five ordinary people.**  They would reasonably reject a principle that allows their lives to be sacrificed for a speculative future benefit.  From their standpoint, the loss is certain and the benefit uncertain, and they have no reason to accept that risk.\n",
      "\n",
      "2. **Perspective of the single potential hero.**  He would also reject a principle that treats him merely as a means to future ends, especially if it entails abandoning five people who are currently in peril.  He could argue that his future contribution is not a guarantee and that a fair contract must protect present persons.\n",
      "\n",
      "3. **Perspective of a reasonable third party.**  Any impartial observer would likely consider the rule “When a group of people faces certain death, they may be sacrificed for a single person whose future benefit is speculative” to be unjust.  The rule would be unstable: it would allow the powerful to sacrifice the vulnerable whenever they claim future importance.\n",
      "\n",
      "4. **Principle that passes the test.**  A principle that says *“Do not sacrifice a known group of people for a speculative future benefit; instead, save those whose lives are presently at stake”* can be reasonably accepted by all parties because it respects each individual's claim to life and avoids using people as mere means.\n",
      "\n",
      "**Conclusion (Contractualism).**  The principle that saves the five ordinary people cannot be reasonably rejected by anyone involved, whereas the principle that sacrifices them for a speculative future benefit would be rejected by the five, the potential hero, and a fair third party.  Therefore, contractualist reasoning supports rescuing the five.\n",
      "\n",
      "---\n",
      "\n",
      "# Summary\n",
      "\n",
      "- **Utilitarianism** – When uncertainty, discounting, and institutional trust are properly accounted for, the expected welfare of saving five known lives exceeds that of a speculative future rescue.  \n",
      "- **Deontology** – The duty to treat each person as an end, the equal intrinsic worth of lives, and the prohibition against instrumentalizing people compel us to save the five.  \n",
      "- **Virtue Ethics** – A virtuous agent, guided by compassion, justice, prudence, and courage, would act to protect the concrete, certain victims.  \n",
      "- **Care Ethics** – The web of immediate relational responsibilities and the ethic of responding to present vulnerability point to rescuing the five.  \n",
      "- **Contractualism** – No one can reasonably accept a principle that sacrifices five for a speculative benefit; a principle protecting the five is universally acceptable.\n",
      "\n",
      "**Thus, across all five major moral frameworks (once each is interpreted faithfully and with attention to its central concerns), the ethically justified decision is to save the five ordinary people.**\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "together = \"\"\n",
    "for index, item in enumerate(results_with_no_name):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += item[\"result\"] + \"\\n\\n\"\n",
    "    \n",
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(results_with_no_name)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "```\n",
    "{together}\n",
    "```\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n",
    "\n",
    "\n",
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"5\", \"4\", \"1\", \"3\", \"2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "# openai = OpenAI()\n",
    "judgment_result = call_openai_compatible(openai, judge_messages, 'gpt-5-mini')\n",
    "\n",
    "print(judgment_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '4', '1', '3', '2']\n",
      "Rank 1: groq\n",
      "Rank 2: deepseek\n",
      "Rank 3: openai\n",
      "Rank 4: gemini\n",
      "Rank 5: anthropic\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "judgement_result_dict = json.loads(judgment_result)\n",
    "ranks = judgement_result_dict[\"results\"]\n",
    "print(ranks)\n",
    "\n",
    "for index, item in enumerate(ranks):\n",
    "    competitor = results_with_names[int(item)-1][\"name\"]\n",
    "    print(f\"Rank {index + 1}: {competitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
